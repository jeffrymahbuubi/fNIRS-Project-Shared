{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae65646",
   "metadata": {},
   "source": [
    "# fNIRS Graph Algorithm Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1f7c6",
   "metadata": {},
   "source": [
    "## Section 1: Data Preparation\n",
    "\n",
    "This section contains:\n",
    "- Dataset class for loading fNIRS data\n",
    "- Feature extraction (node features: 6 statistical features, edge features: correlation + coherence)\n",
    "- Normalization statistics computation\n",
    "- Data transformations (standardization, augmentation)\n",
    "- K-fold data loader creation with subject-level stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95fb31fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu124\n",
      "Cuda available: True\n",
      "Torch geometric version: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning and PyTorch\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torchmetrics\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torchinfo import summary\n",
    "\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import helper_utils\n",
    "\n",
    "# System and File Operations\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Type Hints\n",
    "from typing import Dict, Tuple, Union, List, Optional\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import importlib\n",
    "importlib.reload(helper_utils)\n",
    "helper_utils.set_seed(42)\n",
    "import optuna\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Version Information\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d09c34b",
   "metadata": {},
   "source": [
    "### 1.1 Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfc54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_features(data: np.ndarray, channels_first: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract node features from fNIRS data using statistical measures.\n",
    "\n",
    "    Args:\n",
    "        data: Input fNIRS data array\n",
    "        channels_first: If True, data shape is (C, T), else (T, C)\n",
    "\n",
    "    Returns:\n",
    "        all_node_feats: Tensor of shape (C, F) with statistical features per channel\n",
    "    \"\"\"\n",
    "    stats = helper_utils.compute_statistical_features(data, channels_first=channels_first)\n",
    "\n",
    "    FEATURE_KEYS = (\"mean\", \"min\", \"max\", \"skewness\", \"kurtosis\", \"variance\")\n",
    "\n",
    "    C = len(stats[\"mean\"])\n",
    "    F = len(FEATURE_KEYS)\n",
    "    all_node_feats = np.empty((C, F), dtype=np.float64)\n",
    "\n",
    "    for i in range(C):\n",
    "        all_node_feats[i, 0] = stats[\"mean\"][i]\n",
    "        all_node_feats[i, 1] = stats[\"min\"][i]\n",
    "        all_node_feats[i, 2] = stats[\"max\"][i]\n",
    "        all_node_feats[i, 3] = stats[\"skewness\"][i]\n",
    "        all_node_feats[i, 4] = stats[\"kurtosis\"][i]\n",
    "        all_node_feats[i, 5] = stats[\"variance\"][i]\n",
    "\n",
    "    return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "def get_edge_features(hb_data: np.ndarray, fs: float, directed: bool = False,\n",
    "                     corr_threshold: float = 0.0, self_loops: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Extract edge features from fNIRS data using correlation and coherence.\n",
    "\n",
    "    Args:\n",
    "        hb_data: fNIRS hemoglobin data array\n",
    "        fs: Sampling frequency\n",
    "        directed: If True, create directed graph; if False, undirected\n",
    "        corr_threshold: Minimum absolute correlation threshold for edge inclusion.\n",
    "                       Only edges with |correlation| > corr_threshold are kept.\n",
    "                       Default 0.0 includes all edges.\n",
    "        self_loops: If True, include self-loops (i=j); if False, exclude them.\n",
    "                   Default False excludes self-loops.\n",
    "\n",
    "    Returns:\n",
    "        edge_index: Tensor of shape (2, E) with edge indices\n",
    "        edge_attr: Tensor of shape (E, 2) with [abs_correlation, coherence] features\n",
    "    \"\"\"\n",
    "    # Ensure data is in correct orientation (channels x time)\n",
    "    if hb_data.shape[0] != 23 and hb_data.shape[1] == 23:\n",
    "        hb_data = hb_data.T\n",
    "\n",
    "    # Compute Pearson correlation matrix\n",
    "    R = helper_utils.pearson_correlation_matrix(hb_data, channels_first=True)\n",
    "\n",
    "    # Take absolute value (like fMRI code)\n",
    "    R_abs = np.abs(R)\n",
    "\n",
    "    # Compute coherence matrix\n",
    "    coh_mat, _, _ = helper_utils.coherence_matrix(hb_data, fs=fs, coherence_ratio='1/3',\n",
    "                                                      channels_first=True, return_spectrum=False)\n",
    "\n",
    "    assert R_abs.shape == coh_mat.shape and R_abs.ndim == 2 and R_abs.shape[0] == R_abs.shape[1], \\\n",
    "        \"R_abs and coh_mat must be square matrices of the same shape\"\n",
    "\n",
    "    C = R_abs.shape[0]\n",
    "\n",
    "    if not directed:\n",
    "        # Undirected graph: take upper triangle\n",
    "        k = 0 if self_loops else 1  # k=0 includes diagonal, k=1 excludes it\n",
    "        i, j = np.triu_indices(C, k=k)\n",
    "\n",
    "        # Filter edges: absolute correlation > threshold\n",
    "        mask = R_abs[i, j] > corr_threshold\n",
    "        i, j = i[mask], j[mask]\n",
    "\n",
    "        edge_index = np.vstack([i, j])                    # (2, E)\n",
    "        # Store absolute correlation values\n",
    "        all_edge_feats = np.column_stack([R_abs[i, j], coh_mat[i, j]])  # (E, 2)\n",
    "    else:\n",
    "        # Directed graph: include all pairs (or all except diagonal)\n",
    "        i_list, j_list = [], []\n",
    "        for i in range(C):\n",
    "            for j in range(C):\n",
    "                # Skip self-loops if not enabled\n",
    "                if i == j and not self_loops:\n",
    "                    continue\n",
    "\n",
    "                # Only include edge if absolute correlation > threshold\n",
    "                if R_abs[i, j] > corr_threshold:\n",
    "                    i_list.append(i)\n",
    "                    j_list.append(j)\n",
    "\n",
    "        i = np.asarray(i_list, dtype=int)\n",
    "        j = np.asarray(j_list, dtype=int)\n",
    "        edge_index = np.vstack([i, j])                    # (2, E)\n",
    "        # Store absolute correlation values\n",
    "        all_edge_feats = np.column_stack([R_abs[i, j], coh_mat[i, j]])  # (E, 2)\n",
    "\n",
    "    return torch.tensor(edge_index, dtype=torch.long), torch.tensor(all_edge_feats, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe064c",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e46db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fNIRSGraphDatasetNonRecurrent(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[str, Path],\n",
    "        data_type: str,\n",
    "        max_trials: int,\n",
    "        directed: bool = False,\n",
    "        corr_threshold: float = 0.1,\n",
    "        self_loops: bool = False,\n",
    "        transform=None,\n",
    "        pre_transform=None,\n",
    "        pre_filter=None\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.data_type = data_type\n",
    "        self.max_trials = max_trials\n",
    "        self.directed = directed\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.self_loops = self_loops\n",
    "        self.data_list = []\n",
    "\n",
    "        super().__init__(\n",
    "            root,\n",
    "            transform,\n",
    "            pre_transform,\n",
    "            pre_filter\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return []\n",
    "\n",
    "    def process(self):\n",
    "        base = Path(self.root)\n",
    "        label = {\"healthy\": 0, \"anxiety\": 1}\n",
    "\n",
    "        for label_name in (\"healthy\", \"anxiety\"):\n",
    "            label_dir = base / label_name\n",
    "\n",
    "            for subj_dir in sorted([p for p in label_dir.iterdir() if p.is_dir()]):\n",
    "                subj_id = subj_dir.name\n",
    "\n",
    "                # Load metadata to get sampling frequency\n",
    "                sfreq = None\n",
    "                ini_path = subj_dir / f\"{subj_id}.data\"\n",
    "                if ini_path.exists():\n",
    "                    cfg = configparser.ConfigParser()\n",
    "                    cfg.read(ini_path)\n",
    "                    sfreq = float(cfg[\"metadata\"][\"sfreq\"])\n",
    "                fs = float(sfreq) if (sfreq is not None and np.isfinite(sfreq)) else 10.1\n",
    "\n",
    "                hb_dir = subj_dir / self.data_type\n",
    "                if not hb_dir.exists():\n",
    "                    continue\n",
    "\n",
    "                trial_files = sorted(hb_dir.glob(\"*.npy\"),  key=lambda p: int(p.stem))\n",
    "                if self.max_trials is not None and self.max_trials > 0:\n",
    "                    trial_files = trial_files[: self.max_trials]\n",
    "\n",
    "                for trial_path in trial_files:\n",
    "                    trial_idx = int(trial_path.stem)\n",
    "\n",
    "                    # Load one trial â†’ (C, T) or (T, C)\n",
    "                    arr = np.load(trial_path)\n",
    "                    if arr.ndim != 2:\n",
    "                        raise ValueError(f\"Expected 2D array, got shape {arr.shape} at {trial_path}\")\n",
    "\n",
    "                    # Node features\n",
    "                    node_feats = get_node_features(arr, channels_first=True)\n",
    "\n",
    "                    # Spatial features\n",
    "                    edge_index, edge_feats = get_edge_features(arr, fs=fs, directed=self.directed, corr_threshold=self.corr_threshold, self_loops=self.self_loops)\n",
    "\n",
    "                    # Label\n",
    "                    y = torch.tensor([label[label_name]], dtype=torch.long)\n",
    "\n",
    "                    # Create Data object\n",
    "                    data = Data(\n",
    "                        x=node_feats,\n",
    "                        edge_index=edge_index, edge_attr=edge_feats,\n",
    "                        y=y,\n",
    "                        subject_id=subj_id, trial_idx=trial_idx, label_str=label_name\n",
    "                    )\n",
    "\n",
    "                    # If transform is defined, apply it\n",
    "                    if self.transform is not None:\n",
    "                        data = self.transform(data)\n",
    "\n",
    "                    self.data_list.append(data)\n",
    "\n",
    "    def len(self) -> int:\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def get(self, idx: int) -> Data:\n",
    "        return self.data_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432502e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Keys: ['edge_index', 'label_str', 'y', 'x', 'subject_id', 'trial_idx', 'edge_attr']\n",
      "--------------------------------------------------\n",
      "Node Features Shape: torch.Size([23, 6])\n",
      "Example of Node Features:\n",
      "tensor([[-0.1166, -0.8145,  0.6705,  0.4023,  2.0257,  0.1763],\n",
      "        [-0.5415, -1.1249,  0.3457,  0.5409,  2.3534,  0.1793],\n",
      "        [-0.3092, -2.2508,  1.4274, -0.2847,  1.8703,  1.4145],\n",
      "        [-1.0122, -1.8888, -0.4865, -0.7387,  2.2695,  0.1774],\n",
      "        [ 0.2517, -1.2487,  1.1942, -0.7473,  2.6944,  0.4717],\n",
      "        [ 0.7518, -1.3187,  1.4822, -1.1384,  3.3323,  0.5758],\n",
      "        [ 0.7366, -2.0114,  2.5724, -0.2671,  1.7591,  1.9606],\n",
      "        [ 0.6973, -1.4736,  2.0757, -0.4062,  2.3358,  0.8226],\n",
      "        [ 0.0898, -1.0489,  0.5864, -0.8987,  2.8573,  0.1802],\n",
      "        [ 0.1556, -1.2961,  1.1905, -0.4876,  1.9181,  0.6016],\n",
      "        [-0.8279, -1.3586,  0.0811,  0.7068,  2.3050,  0.1850],\n",
      "        [ 0.3398, -0.2950,  0.7538, -0.5656,  1.7217,  0.1387],\n",
      "        [-0.0591, -1.1298,  0.8704, -0.1273,  1.9576,  0.3223],\n",
      "        [ 1.4651,  0.2694,  2.2696, -0.4365,  1.9994,  0.3291],\n",
      "        [ 0.9979, -0.0742,  2.0800,  0.1687,  2.0195,  0.3891],\n",
      "        [ 0.5951, -1.5220,  2.1063, -0.2912,  1.6066,  1.4722],\n",
      "        [ 0.0258, -1.1268,  0.7680, -0.2511,  1.6678,  0.3542],\n",
      "        [ 0.0453, -0.6698,  0.4530, -0.3921,  1.7291,  0.1027],\n",
      "        [ 0.2405, -0.7936,  0.6024, -1.4003,  4.6177,  0.1059],\n",
      "        [ 0.2526, -1.0044,  1.0343, -0.4092,  1.7004,  0.3820],\n",
      "        [ 1.0949, -0.1086,  2.1473, -0.2427,  1.6299,  0.5295],\n",
      "        [ 0.4815, -0.6404,  1.4053, -0.4228,  1.8362,  0.4312],\n",
      "        [ 0.2652, -0.4046,  0.8840,  0.2862,  1.9607,  0.1217]])\n",
      "--------------------------------------------------\n",
      "Edge Features Shape: torch.Size([479, 2])\n",
      "Example of Edge Features:\n",
      "tensor([[1.0000, 1.0000],\n",
      "        [0.5798, 0.2994],\n",
      "        [0.1645, 0.1484],\n",
      "        [0.2991, 0.0765],\n",
      "        [0.6934, 0.7470],\n",
      "        [0.5799, 0.4974],\n",
      "        [0.6002, 0.5766],\n",
      "        [0.6279, 0.7269],\n",
      "        [0.4540, 0.0322],\n",
      "        [0.1679, 0.2779],\n",
      "        [0.3341, 0.2866],\n",
      "        [0.4565, 0.4402],\n",
      "        [0.5272, 0.3900],\n",
      "        [0.5881, 0.4710],\n",
      "        [0.6036, 0.5644],\n",
      "        [0.5802, 0.6063],\n",
      "        [0.1003, 0.1109],\n",
      "        [0.3138, 0.2617],\n",
      "        [0.1946, 0.2414],\n",
      "        [0.4516, 0.3548],\n",
      "        [0.5798, 0.2994],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.1769, 0.1577],\n",
      "        [0.3607, 0.4132],\n",
      "        [0.1252, 0.1896],\n",
      "        [0.4326, 0.3545],\n",
      "        [0.2553, 0.0867],\n",
      "        [0.2040, 0.0500],\n",
      "        [0.5397, 0.4980],\n",
      "        [0.2238, 0.2440],\n",
      "        [0.7529, 0.3578],\n",
      "        [0.5217, 0.5044],\n",
      "        [0.8056, 0.7589],\n",
      "        [0.7613, 0.7124],\n",
      "        [0.7203, 0.7657],\n",
      "        [0.4013, 0.3330],\n",
      "        [0.3435, 0.2090],\n",
      "        [0.3343, 0.2393],\n",
      "        [0.5944, 0.5181],\n",
      "        [0.2538, 0.3224],\n",
      "        [0.7952, 0.8167],\n",
      "        [0.6696, 0.7489],\n",
      "        [0.8472, 0.9362],\n",
      "        [0.1769, 0.1577],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.9426, 0.8697],\n",
      "        [0.2761, 0.0916],\n",
      "        [0.6119, 0.0491],\n",
      "        [0.5572, 0.0086],\n",
      "        [0.2574, 0.0895],\n",
      "        [0.9856, 0.9722],\n",
      "        [0.9170, 0.8493],\n",
      "        [0.3740, 0.3821],\n",
      "        [0.2339, 0.3995],\n",
      "        [0.1243, 0.1163],\n",
      "        [0.5317, 0.0331],\n",
      "        [0.5612, 0.0353],\n",
      "        [0.9208, 0.8703],\n",
      "        [0.2170, 0.4244],\n",
      "        [0.9499, 0.8680],\n",
      "        [0.5807, 0.4943],\n",
      "        [0.7717, 0.6077],\n",
      "        [0.3162, 0.2900],\n",
      "        [0.1645, 0.1484],\n",
      "        [0.3607, 0.4132],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.6730, 0.4565],\n",
      "        [0.5243, 0.1769],\n",
      "        [0.5101, 0.1675],\n",
      "        [0.7130, 0.5021],\n",
      "        [0.2320, 0.3522],\n",
      "        [0.1413, 0.0133],\n",
      "        [0.2875, 0.4092],\n",
      "        [0.7034, 0.5461],\n",
      "        [0.8284, 0.7236],\n",
      "        [0.7211, 0.4293],\n",
      "        [0.6090, 0.2751],\n",
      "        [0.5380, 0.1786],\n",
      "        [0.3309, 0.4208],\n",
      "        [0.7902, 0.6438],\n",
      "        [0.2630, 0.4279],\n",
      "        [0.6600, 0.6138],\n",
      "        [0.4654, 0.4941],\n",
      "        [0.6682, 0.5346],\n",
      "        [0.2991, 0.0765],\n",
      "        [0.1252, 0.1896],\n",
      "        [0.9426, 0.8697],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.4756, 0.0161],\n",
      "        [0.7354, 0.1236],\n",
      "        [0.7118, 0.0822],\n",
      "        [0.4039, 0.0372],\n",
      "        [0.9190, 0.8287],\n",
      "        [0.8214, 0.7393],\n",
      "        [0.3444, 0.4656],\n",
      "        [0.1225, 0.3503],\n",
      "        [0.1728, 0.1692],\n",
      "        [0.6259, 0.0353],\n",
      "        [0.6605, 0.0558],\n",
      "        [0.7820, 0.6010],\n",
      "        [0.8586, 0.7070],\n",
      "        [0.4903, 0.4821],\n",
      "        [0.6903, 0.5713],\n",
      "        [0.2398, 0.3053],\n",
      "        [0.6934, 0.7470],\n",
      "        [0.4326, 0.3545],\n",
      "        [0.2761, 0.0916],\n",
      "        [0.6730, 0.4565],\n",
      "        [0.4756, 0.0161],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.9156, 0.7116],\n",
      "        [0.9330, 0.7740],\n",
      "        [0.9728, 0.9429],\n",
      "        [0.1673, 0.1630],\n",
      "        [0.4779, 0.3192],\n",
      "        [0.7099, 0.5809],\n",
      "        [0.7969, 0.4418],\n",
      "        [0.9168, 0.6591],\n",
      "        [0.9137, 0.6819],\n",
      "        [0.8623, 0.8047],\n",
      "        [0.1737, 0.1412],\n",
      "        [0.3974, 0.3509],\n",
      "        [0.1825, 0.2818],\n",
      "        [0.5985, 0.4429],\n",
      "        [0.5799, 0.4974],\n",
      "        [0.2553, 0.0867],\n",
      "        [0.6119, 0.0491],\n",
      "        [0.5243, 0.1769],\n",
      "        [0.7354, 0.1236],\n",
      "        [0.9156, 0.7116],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.9899, 0.9360],\n",
      "        [0.9000, 0.6813],\n",
      "        [0.5189, 0.0231],\n",
      "        [0.3705, 0.0128],\n",
      "        [0.2666, 0.0748],\n",
      "        [0.5049, 0.2072],\n",
      "        [0.7209, 0.2696],\n",
      "        [0.9722, 0.8195],\n",
      "        [0.9870, 0.9126],\n",
      "        [0.3414, 0.0122],\n",
      "        [0.6059, 0.2956],\n",
      "        [0.5399, 0.0372],\n",
      "        [0.1855, 0.0118],\n",
      "        [0.3401, 0.1106],\n",
      "        [0.6002, 0.5766],\n",
      "        [0.2040, 0.0500],\n",
      "        [0.5572, 0.0086],\n",
      "        [0.5101, 0.1675],\n",
      "        [0.7118, 0.0822],\n",
      "        [0.9330, 0.7740],\n",
      "        [0.9899, 0.9360],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.8951, 0.6696],\n",
      "        [0.4667, 0.0059],\n",
      "        [0.1032, 0.0889],\n",
      "        [0.3352, 0.0082],\n",
      "        [0.2208, 0.0415],\n",
      "        [0.4813, 0.1809],\n",
      "        [0.6693, 0.1618],\n",
      "        [0.9413, 0.6431],\n",
      "        [0.9661, 0.7810],\n",
      "        [0.2749, 0.0376],\n",
      "        [0.6380, 0.3658],\n",
      "        [0.4922, 0.0085],\n",
      "        [0.1752, 0.0076],\n",
      "        [0.3136, 0.0779],\n",
      "        [0.6279, 0.7269],\n",
      "        [0.5397, 0.4980],\n",
      "        [0.2574, 0.0895],\n",
      "        [0.7130, 0.5021],\n",
      "        [0.4039, 0.0372],\n",
      "        [0.9728, 0.9429],\n",
      "        [0.9000, 0.6813],\n",
      "        [0.8951, 0.6696],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.1373, 0.1681],\n",
      "        [0.1022, 0.0093],\n",
      "        [0.6131, 0.4640],\n",
      "        [0.8037, 0.6979],\n",
      "        [0.9007, 0.6327],\n",
      "        [0.9396, 0.7656],\n",
      "        [0.9257, 0.7440],\n",
      "        [0.8681, 0.7983],\n",
      "        [0.1652, 0.1400],\n",
      "        [0.4916, 0.4593],\n",
      "        [0.2535, 0.3618],\n",
      "        [0.6954, 0.5905],\n",
      "        [0.2238, 0.2440],\n",
      "        [0.9856, 0.9722],\n",
      "        [0.2320, 0.3522],\n",
      "        [0.9190, 0.8287],\n",
      "        [0.1673, 0.1630],\n",
      "        [0.5189, 0.0231],\n",
      "        [0.4667, 0.0059],\n",
      "        [0.1373, 0.1681],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.9380, 0.9151],\n",
      "        [0.4707, 0.4810],\n",
      "        [0.3515, 0.5236],\n",
      "        [0.4285, 0.0217],\n",
      "        [0.4678, 0.0173],\n",
      "        [0.9529, 0.9224],\n",
      "        [0.3362, 0.5539],\n",
      "        [0.9688, 0.9266],\n",
      "        [0.6693, 0.6179],\n",
      "        [0.8333, 0.7174],\n",
      "        [0.4225, 0.4062],\n",
      "        [0.4540, 0.0322],\n",
      "        [0.7529, 0.3578],\n",
      "        [0.1413, 0.0133],\n",
      "        [0.1032, 0.0889],\n",
      "        [0.1022, 0.0093],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.2186, 0.0239],\n",
      "        [0.4488, 0.2057],\n",
      "        [0.2752, 0.0565],\n",
      "        [0.3890, 0.3087],\n",
      "        [0.1403, 0.0769],\n",
      "        [0.3615, 0.1161],\n",
      "        [0.2911, 0.0856],\n",
      "        [0.4366, 0.2075],\n",
      "        [0.1679, 0.2779],\n",
      "        [0.5217, 0.5044],\n",
      "        [0.9170, 0.8493],\n",
      "        [0.2875, 0.4092],\n",
      "        [0.8214, 0.7393],\n",
      "        [0.3705, 0.0128],\n",
      "        [0.3352, 0.0082],\n",
      "        [0.9380, 0.9151],\n",
      "        [0.2186, 0.0239],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.6351, 0.6527],\n",
      "        [0.5220, 0.6792],\n",
      "        [0.1998, 0.3490],\n",
      "        [0.2488, 0.0419],\n",
      "        [0.3020, 0.0278],\n",
      "        [0.9344, 0.8644],\n",
      "        [0.4917, 0.6627],\n",
      "        [0.9348, 0.9117],\n",
      "        [0.8304, 0.8223],\n",
      "        [0.9508, 0.9145],\n",
      "        [0.6445, 0.6540],\n",
      "        [0.3341, 0.2866],\n",
      "        [0.8056, 0.7589],\n",
      "        [0.3740, 0.3821],\n",
      "        [0.7034, 0.5461],\n",
      "        [0.3444, 0.4656],\n",
      "        [0.4779, 0.3192],\n",
      "        [0.2666, 0.0748],\n",
      "        [0.2208, 0.0415],\n",
      "        [0.6131, 0.4640],\n",
      "        [0.4707, 0.4810],\n",
      "        [0.4488, 0.2057],\n",
      "        [0.6351, 0.6527],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.9441, 0.9069],\n",
      "        [0.8429, 0.8570],\n",
      "        [0.4494, 0.3091],\n",
      "        [0.3729, 0.1961],\n",
      "        [0.5111, 0.3868],\n",
      "        [0.7371, 0.5729],\n",
      "        [0.4395, 0.4749],\n",
      "        [0.9320, 0.9017],\n",
      "        [0.7859, 0.7822],\n",
      "        [0.9262, 0.8780],\n",
      "        [0.4565, 0.4402],\n",
      "        [0.7613, 0.7124],\n",
      "        [0.2339, 0.3995],\n",
      "        [0.8284, 0.7236],\n",
      "        [0.1225, 0.3503],\n",
      "        [0.7099, 0.5809],\n",
      "        [0.5049, 0.2072],\n",
      "        [0.4813, 0.1809],\n",
      "        [0.8037, 0.6979],\n",
      "        [0.3515, 0.5236],\n",
      "        [0.2752, 0.0565],\n",
      "        [0.5220, 0.6792],\n",
      "        [0.9441, 0.9069],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.9046, 0.8024],\n",
      "        [0.6364, 0.4196],\n",
      "        [0.5765, 0.3149],\n",
      "        [0.4727, 0.5285],\n",
      "        [0.8933, 0.8228],\n",
      "        [0.3316, 0.5146],\n",
      "        [0.8820, 0.8923],\n",
      "        [0.6899, 0.7720],\n",
      "        [0.9107, 0.8687],\n",
      "        [0.5272, 0.3900],\n",
      "        [0.7203, 0.7657],\n",
      "        [0.1243, 0.1163],\n",
      "        [0.7211, 0.4293],\n",
      "        [0.1728, 0.1692],\n",
      "        [0.7969, 0.4418],\n",
      "        [0.7209, 0.2696],\n",
      "        [0.6693, 0.1618],\n",
      "        [0.9007, 0.6327],\n",
      "        [0.3890, 0.3087],\n",
      "        [0.1998, 0.3490],\n",
      "        [0.8429, 0.8570],\n",
      "        [0.9046, 0.8024],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.8506, 0.6368],\n",
      "        [0.8022, 0.4931],\n",
      "        [0.7856, 0.4935],\n",
      "        [0.6717, 0.6829],\n",
      "        [0.4245, 0.5221],\n",
      "        [0.8377, 0.8142],\n",
      "        [0.5881, 0.4710],\n",
      "        [0.4013, 0.3330],\n",
      "        [0.5317, 0.0331],\n",
      "        [0.6090, 0.2751],\n",
      "        [0.6259, 0.0353],\n",
      "        [0.9168, 0.6591],\n",
      "        [0.9722, 0.8195],\n",
      "        [0.9413, 0.6431],\n",
      "        [0.9396, 0.7656],\n",
      "        [0.4285, 0.0217],\n",
      "        [0.1403, 0.0769],\n",
      "        [0.2488, 0.0419],\n",
      "        [0.4494, 0.3091],\n",
      "        [0.6364, 0.4196],\n",
      "        [0.8506, 0.6368],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.9918, 0.9490],\n",
      "        [0.2827, 0.0256],\n",
      "        [0.6620, 0.3559],\n",
      "        [0.4544, 0.0225],\n",
      "        [0.2459, 0.1974],\n",
      "        [0.5027, 0.3569],\n",
      "        [0.6036, 0.5644],\n",
      "        [0.3435, 0.2090],\n",
      "        [0.5612, 0.0353],\n",
      "        [0.5380, 0.1786],\n",
      "        [0.6605, 0.0558],\n",
      "        [0.9137, 0.6819],\n",
      "        [0.9870, 0.9126],\n",
      "        [0.9661, 0.7810],\n",
      "        [0.9257, 0.7440],\n",
      "        [0.4678, 0.0173],\n",
      "        [0.3020, 0.0278],\n",
      "        [0.3729, 0.1961],\n",
      "        [0.5765, 0.3149],\n",
      "        [0.8022, 0.4931],\n",
      "        [0.9918, 0.9490],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.3125, 0.0190],\n",
      "        [0.6304, 0.3195],\n",
      "        [0.5097, 0.0327],\n",
      "        [0.1653, 0.1075],\n",
      "        [0.1084, 0.0505],\n",
      "        [0.4272, 0.2344],\n",
      "        [0.3343, 0.2393],\n",
      "        [0.9208, 0.8703],\n",
      "        [0.3309, 0.4208],\n",
      "        [0.7820, 0.6010],\n",
      "        [0.3414, 0.0122],\n",
      "        [0.2749, 0.0376],\n",
      "        [0.9529, 0.9224],\n",
      "        [0.9344, 0.8644],\n",
      "        [0.5111, 0.3868],\n",
      "        [0.4727, 0.5285],\n",
      "        [0.2827, 0.0256],\n",
      "        [0.3125, 0.0190],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.5175, 0.6934],\n",
      "        [0.9425, 0.8845],\n",
      "        [0.7312, 0.5760],\n",
      "        [0.8537, 0.6724],\n",
      "        [0.5023, 0.3900],\n",
      "        [0.5802, 0.6063],\n",
      "        [0.5944, 0.5181],\n",
      "        [0.2170, 0.4244],\n",
      "        [0.7902, 0.6438],\n",
      "        [0.8623, 0.8047],\n",
      "        [0.6059, 0.2956],\n",
      "        [0.6380, 0.3658],\n",
      "        [0.8681, 0.7983],\n",
      "        [0.3362, 0.5539],\n",
      "        [0.4917, 0.6627],\n",
      "        [0.7371, 0.5729],\n",
      "        [0.8933, 0.8228],\n",
      "        [0.7856, 0.4935],\n",
      "        [0.6620, 0.3559],\n",
      "        [0.6304, 0.3195],\n",
      "        [0.5175, 0.6934],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.3321, 0.5355],\n",
      "        [0.7636, 0.7009],\n",
      "        [0.6275, 0.6696],\n",
      "        [0.8271, 0.6804],\n",
      "        [0.1003, 0.1109],\n",
      "        [0.2538, 0.3224],\n",
      "        [0.9499, 0.8680],\n",
      "        [0.2630, 0.4279],\n",
      "        [0.8586, 0.7070],\n",
      "        [0.1737, 0.1412],\n",
      "        [0.5399, 0.0372],\n",
      "        [0.4922, 0.0085],\n",
      "        [0.1652, 0.1400],\n",
      "        [0.9688, 0.9266],\n",
      "        [0.9348, 0.9117],\n",
      "        [0.4395, 0.4749],\n",
      "        [0.3316, 0.5146],\n",
      "        [0.4544, 0.0225],\n",
      "        [0.5097, 0.0327],\n",
      "        [0.9425, 0.8845],\n",
      "        [0.3321, 0.5355],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.6843, 0.6839],\n",
      "        [0.8516, 0.7881],\n",
      "        [0.4313, 0.4562],\n",
      "        [0.3138, 0.2617],\n",
      "        [0.7952, 0.8167],\n",
      "        [0.5807, 0.4943],\n",
      "        [0.6600, 0.6138],\n",
      "        [0.4903, 0.4821],\n",
      "        [0.3974, 0.3509],\n",
      "        [0.4916, 0.4593],\n",
      "        [0.6693, 0.6179],\n",
      "        [0.3615, 0.1161],\n",
      "        [0.8304, 0.8223],\n",
      "        [0.9320, 0.9017],\n",
      "        [0.8820, 0.8923],\n",
      "        [0.6717, 0.6829],\n",
      "        [0.2459, 0.1974],\n",
      "        [0.1653, 0.1075],\n",
      "        [0.7312, 0.5760],\n",
      "        [0.7636, 0.7009],\n",
      "        [0.6843, 0.6839],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.9426, 0.9563],\n",
      "        [0.9322, 0.9302],\n",
      "        [0.1946, 0.2414],\n",
      "        [0.6696, 0.7489],\n",
      "        [0.7717, 0.6077],\n",
      "        [0.4654, 0.4941],\n",
      "        [0.6903, 0.5713],\n",
      "        [0.1825, 0.2818],\n",
      "        [0.1855, 0.0118],\n",
      "        [0.1752, 0.0076],\n",
      "        [0.2535, 0.3618],\n",
      "        [0.8333, 0.7174],\n",
      "        [0.2911, 0.0856],\n",
      "        [0.9508, 0.9145],\n",
      "        [0.7859, 0.7822],\n",
      "        [0.6899, 0.7720],\n",
      "        [0.4245, 0.5221],\n",
      "        [0.1084, 0.0505],\n",
      "        [0.8537, 0.6724],\n",
      "        [0.6275, 0.6696],\n",
      "        [0.8516, 0.7881],\n",
      "        [0.9426, 0.9563],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.8286, 0.8620],\n",
      "        [0.4516, 0.3548],\n",
      "        [0.8472, 0.9362],\n",
      "        [0.3162, 0.2900],\n",
      "        [0.6682, 0.5346],\n",
      "        [0.2398, 0.3053],\n",
      "        [0.5985, 0.4429],\n",
      "        [0.3401, 0.1106],\n",
      "        [0.3136, 0.0779],\n",
      "        [0.6954, 0.5905],\n",
      "        [0.4225, 0.4062],\n",
      "        [0.4366, 0.2075],\n",
      "        [0.6445, 0.6540],\n",
      "        [0.9262, 0.8780],\n",
      "        [0.9107, 0.8687],\n",
      "        [0.8377, 0.8142],\n",
      "        [0.5027, 0.3569],\n",
      "        [0.4272, 0.2344],\n",
      "        [0.5023, 0.3900],\n",
      "        [0.8271, 0.6804],\n",
      "        [0.4313, 0.4562],\n",
      "        [0.9322, 0.9302],\n",
      "        [0.8286, 0.8620],\n",
      "        [1.0000, 1.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "root_dir = r'../data/processed_data_HOMER3/GNG'\n",
    "DATA_TYPE = 'hbo'\n",
    "MAX_TRIALS = 2\n",
    "dataset = fNIRSGraphDatasetNonRecurrent(\n",
    "    root=root_dir,\n",
    "    data_type=DATA_TYPE,\n",
    "    max_trials=MAX_TRIALS,\n",
    "    directed=True,\n",
    "    corr_threshold=0.1,\n",
    "    self_loops=True\n",
    ")\n",
    "\n",
    "data = dataset[1]\n",
    "print(f\"Dataset Keys: {data.keys()}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Node Features Shape: {data.x.shape}\")\n",
    "print(f\"Example of Node Features:\\n{data.x}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Edge Features Shape: {data.edge_attr.shape}\")\n",
    "print(f\"Example of Edge Features:\\n{data.edge_attr}\")\n",
    "\n",
    "# sample_data.edge_attr to txt\n",
    "tx = data.edge_attr.numpy()\n",
    "np.savetxt('edge_attr.txt', tx, fmt='%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65129845",
   "metadata": {},
   "source": [
    "### 1.3 Normalization Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab770d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(dataset):\n",
    "    \"\"\"\n",
    "    Compute dataset-wide mean and std for node and edge features.\n",
    "\n",
    "    Args:\n",
    "        dataset: PyTorch Geometric Dataset\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mean_dict, std_dict) with keys 'node_features' and 'edge_features'\n",
    "    \"\"\"\n",
    "    all_node_features = []\n",
    "    all_edge_features = []\n",
    "\n",
    "    for data in dataset:\n",
    "        all_node_features.append(data.x)\n",
    "        if data.edge_attr is not None and len(data.edge_attr) > 0:\n",
    "            all_edge_features.append(data.edge_attr)\n",
    "\n",
    "    # Concatenate all features\n",
    "    all_node_features = torch.cat(all_node_features, dim=0)\n",
    "    all_edge_features = torch.cat(all_edge_features, dim=0) if all_edge_features else None\n",
    "\n",
    "    # Compute mean and std\n",
    "    mean_dict = {\n",
    "        'node_features': all_node_features.mean(dim=0),\n",
    "        'edge_features': all_edge_features.mean(dim=0) if all_edge_features is not None else None\n",
    "    }\n",
    "\n",
    "    std_dict = {\n",
    "        'node_features': all_node_features.std(dim=0),\n",
    "        'edge_features': all_edge_features.std(dim=0) if all_edge_features is not None else None\n",
    "    }\n",
    "\n",
    "    return mean_dict, std_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b7c13",
   "metadata": {},
   "source": [
    "### 1.4 Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ddfb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform, AddRandomWalkPE, Compose\n",
    "from torch_geometric.utils import dropout_edge, mask_feature\n",
    "\n",
    "class StandardizeGraphFeatures(BaseTransform):\n",
    "    \"\"\"Standardize node and edge features using pre-computed statistics.\"\"\"\n",
    "\n",
    "    def __init__(self, mean_dict, std_dict, eps=1e-8):\n",
    "        self.mean_dict = mean_dict\n",
    "        self.std_dict = std_dict\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Standardize node features\n",
    "        if data.x is not None and self.mean_dict['node_features'] is not None:\n",
    "            data.x = (data.x - self.mean_dict['node_features']) / (self.std_dict['node_features'] + self.eps)\n",
    "\n",
    "        # Standardize edge features\n",
    "        if data.edge_attr is not None and self.mean_dict['edge_features'] is not None:\n",
    "            data.edge_attr = (data.edge_attr - self.mean_dict['edge_features']) / (self.std_dict['edge_features'] + self.eps)\n",
    "\n",
    "        return data\n",
    "\n",
    "class MaskFeatureAugmentation(BaseTransform):\n",
    "    \"\"\"Randomly mask features for data augmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, p=0.1, mode='all', fill_value=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p: probability of masking\n",
    "            mode: 'all' (individual values), 'col' (entire features), 'row' (entire nodes)\n",
    "            fill_value: value to fill masked positions\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        self.mode = mode\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "    def forward(self, data):\n",
    "        if data.x is None or self.p == 0:\n",
    "            return data\n",
    "\n",
    "        x = data.x.clone()\n",
    "\n",
    "        if self.mode == 'all':\n",
    "            # Mask individual values\n",
    "            mask = torch.rand_like(x) < self.p\n",
    "            x[mask] = self.fill_value\n",
    "        elif self.mode == 'col':\n",
    "            # Mask entire features (columns)\n",
    "            n_features = x.shape[1]\n",
    "            mask = torch.rand(n_features) < self.p\n",
    "            x[:, mask] = self.fill_value\n",
    "        elif self.mode == 'row':\n",
    "            # Mask entire nodes (rows)\n",
    "            n_nodes = x.shape[0]\n",
    "            mask = torch.rand(n_nodes) < self.p\n",
    "            x[mask, :] = self.fill_value\n",
    "\n",
    "        data.x = x\n",
    "        return data\n",
    "\n",
    "class DropoutEdgeAugmentation(BaseTransform):\n",
    "    \"\"\"Randomly drop edges for data augmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, p=0.1, force_undirected=True):\n",
    "        self.p = p\n",
    "        self.force_undirected = force_undirected\n",
    "\n",
    "    def forward(self, data):\n",
    "        if data.edge_index is None or self.p == 0:\n",
    "            return data\n",
    "\n",
    "        edge_index, edge_attr = dropout_edge(\n",
    "            data.edge_index,\n",
    "            data.edge_attr,\n",
    "            p=self.p,\n",
    "            force_undirected=self.force_undirected\n",
    "        )\n",
    "\n",
    "        data.edge_index = edge_index\n",
    "        data.edge_attr = edge_attr\n",
    "        return data\n",
    "\n",
    "class RandomWalkPEAugmentation(BaseTransform):\n",
    "    \"\"\"Add Random Walk Positional Encoding to node features.\"\"\"\n",
    "\n",
    "    def __init__(self, walk_length=4, attr_name=None):\n",
    "        self.walk_length = walk_length\n",
    "        self.attr_name = attr_name\n",
    "\n",
    "    def forward(self, data):\n",
    "        from torch_geometric.transforms import AddRandomWalkPE\n",
    "\n",
    "        # Apply random walk PE\n",
    "        rw_transform = AddRandomWalkPE(walk_length=self.walk_length, attr_name=self.attr_name)\n",
    "        data = rw_transform(data)\n",
    "\n",
    "        # Concatenate PE to node features\n",
    "        if self.attr_name:\n",
    "            pe = getattr(data, self.attr_name)\n",
    "        else:\n",
    "            pe = data.random_walk_pe\n",
    "\n",
    "        data.x = torch.cat([data.x, pe], dim=-1)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39d172a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformations(mean, std, augment=False,\n",
    "                       edge_dropout_p=0.0, feature_mask_p=0.0, feature_mask_mode='all',\n",
    "                       use_positional_encoding=False, pe_walk_length=4):\n",
    "    \"\"\"\n",
    "    Create transformation pipeline for graphs.\n",
    "\n",
    "    Args:\n",
    "        mean: mean dictionary from get_mean_std\n",
    "        std: std dictionary from get_mean_std\n",
    "        augment: if True, apply data augmentation\n",
    "        edge_dropout_p: probability of edge dropout (default: 0.0)\n",
    "        feature_mask_p: probability of feature masking (default: 0.0)\n",
    "        feature_mask_mode: mode for feature masking ('all', 'col', 'row')\n",
    "        use_positional_encoding: if True, add Random Walk PE\n",
    "        pe_walk_length: walk length for positional encoding\n",
    "\n",
    "    Returns:\n",
    "        Compose object with transformations\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "\n",
    "    # 1. Standardization (always first)\n",
    "    transforms.append(StandardizeGraphFeatures(mean, std))\n",
    "\n",
    "    # 2. Positional Encoding (if enabled, before augmentation)\n",
    "    if use_positional_encoding:\n",
    "        transforms.append(RandomWalkPEAugmentation(walk_length=pe_walk_length))\n",
    "\n",
    "    # 3. Data Augmentation (only for training)\n",
    "    if augment:\n",
    "        if edge_dropout_p > 0:\n",
    "            transforms.append(DropoutEdgeAugmentation(p=edge_dropout_p))\n",
    "        if feature_mask_p > 0:\n",
    "            transforms.append(MaskFeatureAugmentation(p=feature_mask_p, mode=feature_mask_mode))\n",
    "\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749eb25",
   "metadata": {},
   "source": [
    "### 1.5 Load Dataset and Create K-Fold Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a8aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset size: 106\n",
      "Number of node features: 6\n",
      "Number of edge features: 2\n",
      "Number of classes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_ROOT = \"../data/processed_data_HOMER3/GNG\"\n",
    "DATA_TYPE = \"hbo\"\n",
    "MAX_TRIALS = 2\n",
    "DIRECTED = True\n",
    "CORR_THRESHOLD = 0.1\n",
    "SELF_LOOPS = True\n",
    "BATCH_SIZE = 8\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Augmentation parameters (recommended values)\n",
    "EDGE_DROPOUT_P = 0.0\n",
    "FEATURE_MASK_P = 0.1\n",
    "FEATURE_MASK_MODE = 'all'\n",
    "USE_POSITIONAL_ENCODING = False  # Set to True to enable RWPE\n",
    "PE_WALK_LENGTH = 4\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = fNIRSGraphDatasetNonRecurrent(\n",
    "    root=DATA_ROOT,\n",
    "    data_type=DATA_TYPE,\n",
    "    max_trials=MAX_TRIALS,\n",
    "    directed=DIRECTED,\n",
    "    corr_threshold=CORR_THRESHOLD,\n",
    "    self_loops=SELF_LOOPS\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of node features: {dataset[0].x.shape[1]}\")\n",
    "print(f\"Number of edge features: {dataset[0].edge_attr.shape[1] if dataset[0].edge_attr is not None else 0}\")\n",
    "print(f\"Number of classes: {len(set([data.y.item() for data in dataset]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a46384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing normalization statistics...\n",
      "Node features mean: tensor([ 0.0180, -1.0680,  0.9679, -0.1545,  2.4034,  0.5546])\n",
      "Node features std: tensor([0.7270, 1.1150, 0.9750, 0.6541, 0.9562, 0.8711])\n",
      "Edge features mean: tensor([0.7332, 0.5796])\n",
      "Edge features std: tensor([0.2519, 0.3105])\n"
     ]
    }
   ],
   "source": [
    "# Compute normalization statistics\n",
    "print(\"Computing normalization statistics...\")\n",
    "mean_dict, std_dict = get_mean_std(dataset)\n",
    "\n",
    "print(f\"Node features mean: {mean_dict['node_features']}\")\n",
    "print(f\"Node features std: {std_dict['node_features']}\")\n",
    "print(f\"Edge features mean: {mean_dict['edge_features']}\")\n",
    "print(f\"Edge features std: {std_dict['edge_features']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0758219c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations created:\n",
      "Train: Compose([\n",
      "  StandardizeGraphFeatures()\n",
      "])\n",
      "Validation: Compose([\n",
      "  StandardizeGraphFeatures()\n",
      "])\n"
     ]
    }
   ],
   "source": [
    "# Create transformations for train and validation\n",
    "train_transform = get_transformations(\n",
    "    mean_dict, std_dict,\n",
    "    augment=False,  # No augmentation for validation\n",
    ")\n",
    "\n",
    "val_transform = get_transformations(\n",
    "    mean_dict, std_dict,\n",
    "    augment=False,  # No augmentation for validation\n",
    ")\n",
    "\n",
    "print(\"Transformations created:\")\n",
    "print(f\"Train: {train_transform}\")\n",
    "print(f\"Validation: {val_transform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f7392f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Overview ===\n",
      "Total graphs           : 106\n",
      "Unique subjects        : 53\n",
      "Node feature x shape   : (23, 6) | dtype=torch.float32\n",
      "Edge_attr shape        : (475, 2) | dtype=torch.float32\n",
      "Per-graph label counts : {0: 66, 1: 40}\n",
      "Per-subject label cnts : {0: 33, 1: 20}\n",
      "------------------------\n",
      "=== K-Fold 1/5 (by subject) with Transform ===\n",
      "Train\n",
      "  graphs: 84 | subjects: 42\n",
      "  label counts (graphs): {0: 52, 1: 32}\n",
      "  label counts (subjects): {1: 16, 0: 26}\n",
      "  subjects: ['AA013', 'AA041', 'AA064', 'AH014', 'AH015', 'AH017', 'AH018', 'AH019', 'AH021', 'AH022', 'AH024', 'AH025', 'AH026', 'AH027', 'AH029', 'AH033', 'AH034', 'AH035', 'AH036', 'AH037', 'AH038', 'AH039', 'AH040', 'AH043', 'AH044', 'AH045', 'AH047', 'AH048', 'AH049', 'EA012', 'EA016', 'EA060', 'EA061', 'EA062', 'LA051', 'LA052', 'LA053', 'LA054', 'LA057', 'LA058', 'LA059', 'LA063']\n",
      "Val  \n",
      "  graphs: 22 | subjects: 11\n",
      "  label counts (graphs): {0: 14, 1: 8}\n",
      "  label counts (subjects): {1: 4, 0: 7}\n",
      "  subjects: ['AA011', 'AA056', 'AH020', 'AH023', 'AH028', 'AH030', 'AH031', 'AH046', 'AH050', 'EA055', 'LA042']\n",
      "-------------------------------------------------------------\n",
      "=== K-Fold 2/5 (by subject) with Transform ===\n",
      "Train\n",
      "  graphs: 84 | subjects: 42\n",
      "  label counts (graphs): {0: 52, 1: 32}\n",
      "  label counts (subjects): {0: 26, 1: 16}\n",
      "  subjects: ['AA011', 'AA013', 'AA041', 'AA056', 'AA064', 'AH014', 'AH019', 'AH020', 'AH021', 'AH022', 'AH023', 'AH024', 'AH027', 'AH028', 'AH029', 'AH030', 'AH031', 'AH033', 'AH034', 'AH035', 'AH036', 'AH037', 'AH039', 'AH040', 'AH043', 'AH044', 'AH046', 'AH047', 'AH048', 'AH049', 'AH050', 'EA016', 'EA055', 'EA060', 'EA061', 'EA062', 'LA042', 'LA051', 'LA052', 'LA054', 'LA058', 'LA059']\n",
      "Val  \n",
      "  graphs: 22 | subjects: 11\n",
      "  label counts (graphs): {0: 14, 1: 8}\n",
      "  label counts (subjects): {0: 7, 1: 4}\n",
      "  subjects: ['AH015', 'AH017', 'AH018', 'AH025', 'AH026', 'AH038', 'AH045', 'EA012', 'LA053', 'LA057', 'LA063']\n",
      "-------------------------------------------------------------\n",
      "=== K-Fold 3/5 (by subject) with Transform ===\n",
      "Train\n",
      "  graphs: 84 | subjects: 42\n",
      "  label counts (graphs): {0: 52, 1: 32}\n",
      "  label counts (subjects): {1: 16, 0: 26}\n",
      "  subjects: ['AA011', 'AA013', 'AA041', 'AA056', 'AH015', 'AH017', 'AH018', 'AH019', 'AH020', 'AH022', 'AH023', 'AH024', 'AH025', 'AH026', 'AH027', 'AH028', 'AH029', 'AH030', 'AH031', 'AH033', 'AH035', 'AH038', 'AH040', 'AH043', 'AH044', 'AH045', 'AH046', 'AH048', 'AH049', 'AH050', 'EA012', 'EA055', 'EA060', 'EA061', 'EA062', 'LA042', 'LA052', 'LA053', 'LA054', 'LA057', 'LA059', 'LA063']\n",
      "Val  \n",
      "  graphs: 22 | subjects: 11\n",
      "  label counts (graphs): {0: 14, 1: 8}\n",
      "  label counts (subjects): {0: 7, 1: 4}\n",
      "  subjects: ['AA064', 'AH014', 'AH021', 'AH034', 'AH036', 'AH037', 'AH039', 'AH047', 'EA016', 'LA051', 'LA058']\n",
      "-------------------------------------------------------------\n",
      "=== K-Fold 4/5 (by subject) with Transform ===\n",
      "Train\n",
      "  graphs: 86 | subjects: 43\n",
      "  label counts (graphs): {0: 54, 1: 32}\n",
      "  label counts (subjects): {1: 16, 0: 27}\n",
      "  subjects: ['AA011', 'AA013', 'AA056', 'AA064', 'AH014', 'AH015', 'AH017', 'AH018', 'AH020', 'AH021', 'AH022', 'AH023', 'AH024', 'AH025', 'AH026', 'AH028', 'AH030', 'AH031', 'AH034', 'AH035', 'AH036', 'AH037', 'AH038', 'AH039', 'AH040', 'AH045', 'AH046', 'AH047', 'AH048', 'AH049', 'AH050', 'EA012', 'EA016', 'EA055', 'EA060', 'EA061', 'LA042', 'LA051', 'LA052', 'LA053', 'LA057', 'LA058', 'LA063']\n",
      "Val  \n",
      "  graphs: 20 | subjects: 10\n",
      "  label counts (graphs): {0: 12, 1: 8}\n",
      "  label counts (subjects): {1: 4, 0: 6}\n",
      "  subjects: ['AA041', 'AH019', 'AH027', 'AH029', 'AH033', 'AH043', 'AH044', 'EA062', 'LA054', 'LA059']\n",
      "-------------------------------------------------------------\n",
      "=== K-Fold 5/5 (by subject) with Transform ===\n",
      "Train\n",
      "  graphs: 86 | subjects: 43\n",
      "  label counts (graphs): {0: 54, 1: 32}\n",
      "  label counts (subjects): {1: 16, 0: 27}\n",
      "  subjects: ['AA011', 'AA041', 'AA056', 'AA064', 'AH014', 'AH015', 'AH017', 'AH018', 'AH019', 'AH020', 'AH021', 'AH023', 'AH025', 'AH026', 'AH027', 'AH028', 'AH029', 'AH030', 'AH031', 'AH033', 'AH034', 'AH036', 'AH037', 'AH038', 'AH039', 'AH043', 'AH044', 'AH045', 'AH046', 'AH047', 'AH050', 'EA012', 'EA016', 'EA055', 'EA062', 'LA042', 'LA051', 'LA053', 'LA054', 'LA057', 'LA058', 'LA059', 'LA063']\n",
      "Val  \n",
      "  graphs: 20 | subjects: 10\n",
      "  label counts (graphs): {0: 12, 1: 8}\n",
      "  label counts (subjects): {0: 6, 1: 4}\n",
      "  subjects: ['AA013', 'AH022', 'AH024', 'AH035', 'AH040', 'AH048', 'AH049', 'EA060', 'EA061', 'LA052']\n",
      "-------------------------------------------------------------\n",
      "\n",
      "Total folds: 5\n",
      "\n",
      "Fold 1:\n",
      "  Train loader batches: 11\n",
      "  Val loader batches: 3\n",
      "\n",
      "  Batch from fold 1 train_loader:\n",
      "    Number of graphs: 8\n",
      "    Node features shape: torch.Size([184, 6])\n",
      "    Edge features shape: torch.Size([4082, 2])\n",
      "    Features are standardized (means near 0): tensor([-0.0722, -0.3720,  0.1692])\n"
     ]
    }
   ],
   "source": [
    "# Create K-fold data loaders\n",
    "fold_loaders_v2 = helper_utils.get_kfold_subject_loaders_v2(\n",
    "    dataset,\n",
    "    n_splits=5,\n",
    "    batch_size=8,\n",
    "    shuffle_train=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    random_state=42,\n",
    "    show_subjects=True,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal folds: {len(fold_loaders_v2)}\")\n",
    "\n",
    "# Test first fold\n",
    "train_loader_fold1, val_loader_fold1 = fold_loaders_v2[0]\n",
    "print(f\"\\nFold 1:\")\n",
    "print(f\"  Train loader batches: {len(train_loader_fold1)}\")\n",
    "print(f\"  Val loader batches: {len(val_loader_fold1)}\")\n",
    "\n",
    "# Test a batch from first fold\n",
    "batch = next(iter(train_loader_fold1))\n",
    "print(f\"\\n  Batch from fold 1 train_loader:\")\n",
    "print(f\"    Number of graphs: {batch.num_graphs}\")\n",
    "print(f\"    Node features shape: {batch.x.shape}\")\n",
    "print(f\"    Edge features shape: {batch.edge_attr.shape}\")\n",
    "print(f\"    Features are standardized (means near 0): {batch.x.mean(dim=0)[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7578a0",
   "metadata": {},
   "source": [
    "## Section 2: Graph Model Initialization\n",
    "\n",
    "This section contains:\n",
    "- Flexible GAT model architecture\n",
    "- Class weight calculation for handling class imbalance\n",
    "- Loss function options (Focal Loss and weighted Cross Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7cb6b",
   "metadata": {},
   "source": [
    "### 2.1 Flexible GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9150146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, LayerNorm, ReLU\n",
    "from torch_geometric.nn import GATv2Conv, global_mean_pool, GINEConv\n",
    "from typing import Optional, Dict, Any, Union\n",
    "\n",
    "class FlexibleGATNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible GAT Network with configurable architecture.\n",
    "    Supports both pure GAT and GINE-GAT hybrid architectures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, n_layers, n_filters, heads, fc_size,\n",
    "                 dropout=0.6, edge_dim=None, n_classes=2,\n",
    "                 use_residual=True, use_norm=False, norm_type='batch',\n",
    "                 use_gine_first_layer=False, gine_train_eps=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: input node feature dimension\n",
    "            n_layers: number of graph convolution layers\n",
    "            n_filters: hidden dimensions per layer (int or list)\n",
    "            heads: attention heads per layer (int or list)\n",
    "            fc_size: output dimension before classification\n",
    "            dropout: dropout probability\n",
    "            edge_dim: edge feature dimension\n",
    "            n_classes: number of output classes\n",
    "            use_residual: enable residual connections\n",
    "            use_norm: enable normalization between layers\n",
    "            norm_type: 'batch' or 'layer' normalization\n",
    "            use_gine_first_layer: use GINEConv for first layer\n",
    "            gine_train_eps: learnable epsilon for GINE\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_residual = use_residual\n",
    "        self.use_norm = use_norm\n",
    "        self.norm_type = norm_type\n",
    "        self.use_gine_first_layer = use_gine_first_layer\n",
    "\n",
    "        # Handle scalar or list configurations\n",
    "        if isinstance(n_filters, int):\n",
    "            n_filters = [n_filters] * n_layers\n",
    "        if isinstance(heads, int):\n",
    "            heads = [heads] * n_layers\n",
    "\n",
    "        self.n_filters = n_filters\n",
    "        self.heads = heads\n",
    "\n",
    "        # Build graph convolution layers\n",
    "        self.convs, self.residual_projections, self.norms = self._build_gat_layers(\n",
    "            in_channels, n_filters, heads, edge_dim, gine_train_eps\n",
    "        )\n",
    "\n",
    "        # Final classifier\n",
    "        final_dim = n_filters[-1] * heads[-1]\n",
    "        self.fc1 = nn.Linear(final_dim, fc_size)\n",
    "        self.fc2 = nn.Linear(fc_size, n_classes)\n",
    "\n",
    "    def _build_gine_mlp(self, in_dim, out_dim):\n",
    "        \"\"\"Build MLP for GINEConv layer.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def _build_gat_layers(self, in_channels, n_filters, heads, edge_dim, gine_train_eps):\n",
    "        \"\"\"Build GAT/GINE convolution layers with optional residual and normalization.\"\"\"\n",
    "        convs = nn.ModuleList()\n",
    "        residual_projections = nn.ModuleList()\n",
    "        norms = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            in_dim = in_channels if i == 0 else n_filters[i-1] * heads[i-1]\n",
    "            out_dim = n_filters[i]\n",
    "            n_heads = heads[i]\n",
    "\n",
    "            # First layer: GINE or GAT based on configuration\n",
    "            if i == 0 and self.use_gine_first_layer:\n",
    "                mlp = self._build_gine_mlp(in_dim, out_dim * n_heads)\n",
    "                conv = GINEConv(mlp, edge_dim=edge_dim, train_eps=gine_train_eps)\n",
    "            else:\n",
    "                conv = GATv2Conv(\n",
    "                    in_dim, out_dim, heads=n_heads,\n",
    "                    dropout=self.dropout, edge_dim=edge_dim, concat=True\n",
    "                )\n",
    "\n",
    "            convs.append(conv)\n",
    "\n",
    "            # Residual projection (if input and output dimensions differ)\n",
    "            if self.use_residual and in_dim != out_dim * n_heads:\n",
    "                residual_projections.append(nn.Linear(in_dim, out_dim * n_heads))\n",
    "            else:\n",
    "                residual_projections.append(None)\n",
    "\n",
    "            # Normalization\n",
    "            if self.use_norm:\n",
    "                if self.norm_type == 'batch':\n",
    "                    norms.append(nn.BatchNorm1d(out_dim * n_heads))\n",
    "                elif self.norm_type == 'layer':\n",
    "                    norms.append(nn.LayerNorm(out_dim * n_heads))\n",
    "            else:\n",
    "                norms.append(None)\n",
    "\n",
    "        return convs, residual_projections, norms\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: node features [num_nodes, in_channels]\n",
    "            edge_index: edge indices [2, num_edges]\n",
    "            edge_attr: edge features [num_edges, edge_dim]\n",
    "            batch: batch assignment [num_nodes]\n",
    "\n",
    "        Returns:\n",
    "            logits: [batch_size, n_classes]\n",
    "        \"\"\"\n",
    "        # Graph convolution layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            identity = x\n",
    "\n",
    "            # Apply convolution\n",
    "            if isinstance(conv, GINEConv):\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "            else:  # GATv2Conv\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "\n",
    "            # Apply normalization\n",
    "            if self.norms[i] is not None:\n",
    "                x = self.norms[i](x)\n",
    "\n",
    "            # Apply residual connection\n",
    "            if self.use_residual:\n",
    "                if self.residual_projections[i] is not None:\n",
    "                    identity = self.residual_projections[i](identity)\n",
    "                x = x + identity\n",
    "\n",
    "            # Apply activation and dropout (except for last layer)\n",
    "            if i < self.n_layers - 1:\n",
    "                x = F.elu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Classifier\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab2cfa",
   "metadata": {},
   "source": [
    "### 2.2 Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling class imbalance.\n",
    "\n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: class weights tensor or None\n",
    "            gamma: focusing parameter (default: 2.0)\n",
    "            reduction: 'mean', 'sum', or 'none'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: logits [batch_size, n_classes]\n",
    "            targets: labels [batch_size]\n",
    "\n",
    "        Returns:\n",
    "            loss: scalar or tensor depending on reduction\n",
    "        \"\"\"\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def calculate_class_weights(data_source, device, fold_idx=None, use_sqrt=False):\n",
    "    \"\"\"\n",
    "    Calculate balanced class weights for handling class imbalance.\n",
    "\n",
    "    Args:\n",
    "        data_source: DataLoader or list of fold loaders\n",
    "        device: torch device\n",
    "        fold_idx: fold index if using K-fold loaders\n",
    "        use_sqrt: if True, apply square root to reduce aggressive weighting\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: class weights on specified device\n",
    "    \"\"\"\n",
    "    # Extract labels\n",
    "    if fold_idx is not None:\n",
    "        # K-fold loaders\n",
    "        labels = []\n",
    "        for batch in data_source[fold_idx]['train']:\n",
    "            labels.extend(batch.y.cpu().numpy())\n",
    "        labels = np.array(labels)\n",
    "    else:\n",
    "        # Single DataLoader\n",
    "        labels = []\n",
    "        for batch in data_source:\n",
    "            labels.extend(batch.y.cpu().numpy())\n",
    "        labels = np.array(labels)\n",
    "\n",
    "    # Compute class weights\n",
    "    classes = np.unique(labels)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=labels)\n",
    "\n",
    "    # Optional: reduce aggressive weighting with square root\n",
    "    if use_sqrt:\n",
    "        weights = np.sqrt(weights)\n",
    "\n",
    "    return torch.tensor(weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae90592",
   "metadata": {},
   "source": [
    "### 2.3 Model Configuration and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "  in_channels: 6\n",
      "  n_layers: 2\n",
      "  n_filters: [112, 32]\n",
      "  heads: [6, 4]\n",
      "  fc_size: 96\n",
      "  dropout: 0.4\n",
      "  edge_dim: 2\n",
      "  n_classes: 2\n",
      "  use_residual: True\n",
      "  use_norm: True\n",
      "  norm_type: batch\n",
      "  use_gine_first_layer: True\n",
      "  gine_train_eps: True\n",
      "\n",
      "Loss Configuration:\n",
      "  Use Focal Loss: False\n",
      "  Use Class Weights: False\n",
      "  Learning Rate: 0.001\n",
      "  Weight Decay: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "MODEL_CONFIG = {\n",
    "    'in_channels': 6,  # 6 statistical features (or 6 + PE_WALK_LENGTH if using RWPE)\n",
    "    'n_layers': 2,\n",
    "    'n_filters': [112, 32],\n",
    "    'heads': [6, 4],\n",
    "    'fc_size': 96,\n",
    "    'dropout': 0.4,\n",
    "    'edge_dim': 2,  # correlation + coherence\n",
    "    'n_classes': 2,\n",
    "    'use_residual': True,\n",
    "    'use_norm': True,\n",
    "    'norm_type': 'batch',\n",
    "    'use_gine_first_layer': True,\n",
    "    'gine_train_eps': True\n",
    "}\n",
    "\n",
    "# Adjust input channels if using positional encoding\n",
    "if USE_POSITIONAL_ENCODING:\n",
    "    MODEL_CONFIG['in_channels'] = 6 + PE_WALK_LENGTH\n",
    "\n",
    "# Loss function configuration\n",
    "USE_FOCAL_LOSS = False  # Set to True to use Focal Loss\n",
    "FOCAL_GAMMA = 2.0\n",
    "USE_CLASS_WEIGHTS = False\n",
    "USE_SQRT_WEIGHTS = False\n",
    "\n",
    "# Learning rate and optimization\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nLoss Configuration:\")\n",
    "print(f\"  Use Focal Loss: {USE_FOCAL_LOSS}\")\n",
    "print(f\"  Use Class Weights: {USE_CLASS_WEIGHTS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight Decay: {WEIGHT_DECAY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e711594",
   "metadata": {},
   "source": [
    "## Section 3: 5-Fold Cross-Validation Training Pipeline\n",
    "\n",
    "This section contains:\n",
    "- Early stopping mechanism\n",
    "- Training and validation functions\n",
    "- K-fold cross-validation training loop\n",
    "- Metrics tracking and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26467b",
   "metadata": {},
   "source": [
    "### 3.1 Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bed73d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop training when validation metric stops improving.\n",
    "\n",
    "    Args:\n",
    "        patience: Number of epochs to wait for improvement\n",
    "        min_delta: Minimum change to qualify as improvement\n",
    "        mode: 'min' for loss, 'max' for accuracy/F1\n",
    "        verbose: Print messages when stopping\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.0001, mode='min', verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "\n",
    "    def __call__(self, current_score, epoch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            current_score: Current validation metric value\n",
    "            epoch: Current epoch number\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop\n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "\n",
    "        improved = False\n",
    "        if self.mode == 'min':\n",
    "            improved = current_score < (self.best_score - self.min_delta)\n",
    "        else:  # mode == 'max'\n",
    "            improved = current_score > (self.best_score + self.min_delta)\n",
    "\n",
    "        if improved:\n",
    "            self.best_score = current_score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"  EarlyStopping: {self.counter}/{self.patience} epochs without improvement\")\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(f\"  EarlyStopping: Stopping training at epoch {epoch}\")\n",
    "                print(f\"  Best score: {self.best_score:.6f} at epoch {self.best_epoch}\")\n",
    "            self.early_stop = True\n",
    "\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27757644",
   "metadata": {},
   "source": [
    "### 3.2 Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "654740d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_experiment_dir(experiment_name: str, base_dir: str = \"experiments\", overwrite: bool = False) -> str:\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "    exp_dir = os.path.join(base_dir, date_str, experiment_name)\n",
    "    if os.path.exists(exp_dir) and overwrite:\n",
    "        print(f\"Removing existing directory: {exp_dir}\")\n",
    "        shutil.rmtree(exp_dir)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    return exp_dir\n",
    "\n",
    "def save_metrics(metrics_dict, save_dir, filename):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_path = os.path.join(save_dir, f\"{filename}.pkl\")\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(metrics_dict, f)\n",
    "\n",
    "def save_best_model(model, save_dir, model_name):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"{model_name}_best.pt\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"â€¢ Saved best model weights to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "def visualize_training_results(results, save_dir, experiment_name, training_type=\"holdout\", best_epoch=None):\n",
    "    \"\"\"\n",
    "    Visualizes training results and saves figures to the experiment directory.\n",
    "\n",
    "    Compatible with both holdout and k-fold training results, including early stopping.\n",
    "\n",
    "    Args:\n",
    "        results: Results from perform_holdout_training or perform_kfold_training\n",
    "                 - Holdout (new): tuple of (holdout_metrics, final_summary)\n",
    "                 - Holdout (old): dict with \"history\" and \"final\" keys\n",
    "                 - K-fold: dict with \"overall\" and \"folds\" keys\n",
    "        save_dir (str): Directory to save the figures\n",
    "        experiment_name (str): Name of the experiment (for file naming)\n",
    "        training_type (str): Either \"holdout\" or \"kfold\" (default: \"holdout\")\n",
    "        best_epoch (int, optional): Best epoch index (0-indexed) to mark on plots.\n",
    "                                   If None, will try to extract from results.\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to all saved figures\n",
    "            {\n",
    "                \"loss_plot\": str,\n",
    "                \"accuracy_plot\": str,\n",
    "                \"f1_plot\": str,\n",
    "                \"confusion_matrix_plot\": str\n",
    "            }\n",
    "\n",
    "    Examples:\n",
    "        # Holdout training (new structure)\n",
    "        holdout_metrics, final_summary = perform_holdout_training(...)\n",
    "        exp_dir = get_experiment_dir(\"my_experiment\")\n",
    "        paths = visualize_training_results(\n",
    "            (holdout_metrics, final_summary),\n",
    "            exp_dir,\n",
    "            \"my_experiment\",\n",
    "            \"holdout\"\n",
    "        )\n",
    "\n",
    "        # K-fold training\n",
    "        results = perform_kfold_training(...)\n",
    "        exp_dir = get_experiment_dir(\"my_kfold\")\n",
    "        paths = visualize_training_results(\n",
    "            results,\n",
    "            exp_dir,\n",
    "            \"my_kfold\",\n",
    "            \"kfold\"\n",
    "        )\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Extract metrics based on training type and result structure\n",
    "    if training_type == \"holdout\":\n",
    "        # Check if results is a tuple (new structure) or dict (old structure)\n",
    "        if isinstance(results, tuple):\n",
    "            # New structure from updated perform_holdout_training\n",
    "            holdout_metrics, final_summary = results\n",
    "            metrics = holdout_metrics\n",
    "            final_cm = np.array(final_summary[\"confusion_matrix\"])\n",
    "            # Extract best_epoch if not provided\n",
    "            if best_epoch is None:\n",
    "                best_epoch = holdout_metrics.get(\"best_epoch\", None)\n",
    "        else:\n",
    "            # Old structure (backward compatibility)\n",
    "            metrics = results.get(\"history\", results)\n",
    "            final_cm = np.array(results[\"final\"][\"confusion_matrix\"])\n",
    "\n",
    "    elif training_type == \"kfold\":\n",
    "        # K-fold structure remains compatible\n",
    "        metrics = results[\"overall\"]\n",
    "        final_cm = np.array(results[\"overall\"][\"confusion_matrix_overall\"])\n",
    "        # For k-fold aggregate plot, best_epoch doesn't apply directly\n",
    "        # (each fold has its own best epoch)\n",
    "        best_epoch = None\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid training_type: {training_type}. Must be 'holdout' or 'kfold'\")\n",
    "\n",
    "    # Extract metric arrays\n",
    "    train_loss = np.array(metrics[\"train_loss\"])\n",
    "    val_loss = np.array(metrics[\"val_loss\"])\n",
    "    train_acc = np.array(metrics[\"train_accuracy\"])\n",
    "    val_acc = np.array(metrics[\"val_accuracy\"])\n",
    "    train_f1 = np.array(metrics[\"train_f1\"])\n",
    "    val_f1 = np.array(metrics[\"val_f1\"])\n",
    "\n",
    "    # Handle NaN values in k-fold (from variable-length folds due to early stopping)\n",
    "    if training_type == \"kfold\":\n",
    "        # Find first occurrence of NaN to truncate arrays\n",
    "        valid_mask = ~np.isnan(train_loss)\n",
    "        if not np.all(valid_mask):\n",
    "            max_valid_idx = np.where(valid_mask)[0][-1] + 1\n",
    "            train_loss = train_loss[:max_valid_idx]\n",
    "            val_loss = val_loss[:max_valid_idx]\n",
    "            train_acc = train_acc[:max_valid_idx]\n",
    "            val_acc = val_acc[:max_valid_idx]\n",
    "            train_f1 = train_f1[:max_valid_idx]\n",
    "            val_f1 = val_f1[:max_valid_idx]\n",
    "\n",
    "    # Create epoch array (1-indexed for display)\n",
    "    epochs = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    # Get early stopping info for titles\n",
    "    stopped_at = metrics.get(\"stopped_at_epoch\", len(train_loss) - 1)\n",
    "    early_stop_info = \"\"\n",
    "    if best_epoch is not None:\n",
    "        early_stop_info = f\"\\n(Best: Epoch {best_epoch+1}, Stopped: Epoch {stopped_at+1})\"\n",
    "\n",
    "    # Dictionary to store saved paths\n",
    "    saved_paths = {}\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Figure 1: Training vs Validation Loss\n",
    "    # -------------------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(epochs, train_loss, 'b-', label='Training Loss', linewidth=2)\n",
    "    ax.plot(epochs, val_loss, 'r-', label='Validation Loss', linewidth=2)\n",
    "\n",
    "    # Add best epoch marker\n",
    "    if best_epoch is not None and best_epoch < len(val_loss):\n",
    "        ax.axvline(x=best_epoch+1, color='green', linestyle='--',\n",
    "                   linewidth=2, alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
    "        ax.plot(best_epoch+1, val_loss[best_epoch],\n",
    "               'g*', markersize=15, label='Best Val Loss', zorder=5)\n",
    "\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title(f'Training vs Validation Loss - {experiment_name}{early_stop_info}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    loss_path = os.path.join(save_dir, f\"{experiment_name}_loss_curves.png\")\n",
    "    plt.savefig(loss_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    saved_paths[\"loss_plot\"] = loss_path\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Figure 2: Training vs Validation Accuracy\n",
    "    # -------------------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(epochs, train_acc, 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax.plot(epochs, val_acc, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "\n",
    "    # Add best epoch marker\n",
    "    if best_epoch is not None and best_epoch < len(val_acc):\n",
    "        ax.axvline(x=best_epoch+1, color='green', linestyle='--',\n",
    "                   linewidth=2, alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
    "        ax.plot(best_epoch+1, val_acc[best_epoch],\n",
    "               'g*', markersize=15, label='Best Val Acc', zorder=5)\n",
    "\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title(f'Training vs Validation Accuracy - {experiment_name}{early_stop_info}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "    acc_path = os.path.join(save_dir, f\"{experiment_name}_accuracy_curves.png\")\n",
    "    plt.savefig(acc_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    saved_paths[\"accuracy_plot\"] = acc_path\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Figure 3: Training vs Validation F1-Score\n",
    "    # -------------------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(epochs, train_f1, 'b-', label='Training F1-Score', linewidth=2)\n",
    "    ax.plot(epochs, val_f1, 'r-', label='Validation F1-Score', linewidth=2)\n",
    "\n",
    "    # Add best epoch marker\n",
    "    if best_epoch is not None and best_epoch < len(val_f1):\n",
    "        ax.axvline(x=best_epoch+1, color='green', linestyle='--',\n",
    "                   linewidth=2, alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
    "        ax.plot(best_epoch+1, val_f1[best_epoch],\n",
    "               'g*', markersize=15, label='Best Val F1', zorder=5)\n",
    "\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('F1-Score', fontsize=12)\n",
    "    ax.set_title(f'Training vs Validation F1-Score - {experiment_name}{early_stop_info}',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "    f1_path = os.path.join(save_dir, f\"{experiment_name}_f1_curves.png\")\n",
    "    plt.savefig(f1_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    saved_paths[\"f1_plot\"] = f1_path\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Figure 4: Confusion Matrix\n",
    "    # -------------------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Create heatmap\n",
    "    sns.heatmap(final_cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Healthy (0)', 'Anxiety (1)'],\n",
    "                yticklabels=['Healthy (0)', 'Anxiety (1)'],\n",
    "                cbar_kws={'label': 'Count'},\n",
    "                ax=ax)\n",
    "\n",
    "    cm_title = f'Final Confusion Matrix - {experiment_name}'\n",
    "    if best_epoch is not None:\n",
    "        cm_title += f'\\n(From Best Epoch: {best_epoch+1})'\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_title(cm_title, fontsize=14, fontweight='bold')\n",
    "\n",
    "    cm_path = os.path.join(save_dir, f\"{experiment_name}_confusion_matrix.png\")\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    saved_paths[\"confusion_matrix_plot\"] = cm_path\n",
    "\n",
    "    print(f\"\\nVisualization saved to: {save_dir}\")\n",
    "    print(f\"  - Loss curves: {os.path.basename(loss_path)}\")\n",
    "    print(f\"  - Accuracy curves: {os.path.basename(acc_path)}\")\n",
    "    print(f\"  - F1-Score curves: {os.path.basename(f1_path)}\")\n",
    "    print(f\"  - Confusion Matrix: {os.path.basename(cm_path)}\")\n",
    "\n",
    "    if best_epoch is not None:\n",
    "        print(f\"\\nBest model from epoch {best_epoch+1} (marked with green line and star)\")\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn, device,\n",
    "          epoch=None, n_epochs=None, verbose=True, log_freq=10):\n",
    "    \"\"\"\n",
    "    Performs one full training epoch over the train_loader.\n",
    "\n",
    "    Args:\n",
    "        model: The GNN model to train\n",
    "        train_loader: DataLoader for the training set\n",
    "        optimizer: Optimizer for updating model parameters\n",
    "        loss_fn: Loss function (e.g., CrossEntropyLoss)\n",
    "        device: Device to run training on ('cpu' or 'cuda')\n",
    "        epoch: Current epoch number (for logging, optional)\n",
    "        n_epochs: Total number of epochs (for logging, optional)\n",
    "        verbose: If True, print progress (default: True)\n",
    "        log_freq: Log every N batches (default: 10)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (avg_loss, train_acc, train_f1)\n",
    "            - avg_loss: Average training loss over all batches\n",
    "            - train_acc: Training accuracy (computed using torchmetrics)\n",
    "            - train_f1: Training F1 score (computed using torchmetrics)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Introspect model signature once\n",
    "    sig = inspect.signature(model.forward)\n",
    "    param_names = set(sig.parameters.keys())\n",
    "    use_x          = ('x'          in param_names)\n",
    "    use_edge_index = ('edge_index' in param_names)\n",
    "    use_edge_attr  = ('edge_attr'  in param_names)\n",
    "    use_batch      = ('batch'      in param_names)\n",
    "\n",
    "    # Create metrics for this epoch\n",
    "    train_accuracy_metric = torchmetrics.Accuracy(task='binary').to(device)\n",
    "    train_f1_metric = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "    # Determine total batches for logging\n",
    "    total_batches = len(train_loader)\n",
    "    epoch_str = f\"{epoch}/{n_epochs}\" if (epoch is not None and n_epochs is not None) else \"1/1\"\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass - build kwargs based on model signature\n",
    "        optimizer.zero_grad()\n",
    "        kwargs = {}\n",
    "        if use_x:          kwargs['x'] = batch.x\n",
    "        if use_edge_index: kwargs['edge_index'] = batch.edge_index\n",
    "        if use_edge_attr and hasattr(batch, 'edge_attr'):\n",
    "            kwargs['edge_attr'] = batch.edge_attr\n",
    "        if use_batch and hasattr(batch, 'batch'):\n",
    "            kwargs['batch'] = batch.batch\n",
    "        logits = model(**kwargs)\n",
    "\n",
    "        y_gt = batch.y.view(-1).long()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, y_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update metrics with class predictions\n",
    "        batch_preds = torch.argmax(logits, dim=-1)\n",
    "        train_accuracy_metric.update(batch_preds, y_gt)\n",
    "        train_f1_metric.update(batch_preds, y_gt)\n",
    "\n",
    "        # Log progress at specified frequency\n",
    "        current_batch = batch_idx + 1\n",
    "        if verbose and (current_batch % log_freq == 0 or current_batch == total_batches):\n",
    "            avg_loss = total_loss / current_batch\n",
    "            current_acc = train_accuracy_metric.compute().item()\n",
    "            current_f1 = train_f1_metric.compute().item()\n",
    "            print(f\"Epoch [{epoch_str}], Step [{current_batch}/{total_batches}], \"\n",
    "                  f\"Loss: {avg_loss:.4f}, Acc: {current_acc:.4f}, F1: {current_f1:.4f}\")\n",
    "\n",
    "    # Compute final metrics for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_acc = train_accuracy_metric.compute().item()\n",
    "    train_f1 = train_f1_metric.compute().item()\n",
    "\n",
    "    return avg_loss, train_acc, train_f1\n",
    "\n",
    "def val(model, val_loader, loss_fn, device,\n",
    "        epoch=None, n_epochs=None, verbose=True, log_final_only=True):\n",
    "    \"\"\"\n",
    "    Performs one full validation epoch over the val_loader.\n",
    "\n",
    "    Computes all validation metrics internally including precision, recall,\n",
    "    and confusion matrix, eliminating the need for a separate logger class.\n",
    "\n",
    "    Args:\n",
    "        model: The GNN model to evaluate\n",
    "        val_loader: DataLoader for the validation set\n",
    "        loss_fn: Loss function (e.g., CrossEntropyLoss)\n",
    "        device: Device to run evaluation on ('cpu' or 'cuda')\n",
    "        epoch: Current epoch number (for logging, optional)\n",
    "        n_epochs: Total number of epochs (for logging, optional)\n",
    "        verbose: If True, print results (default: True)\n",
    "        log_final_only: If True, only log final results after all batches (default: True)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (avg_loss, val_acc, val_f1, precision, recall, cm, preds, targets)\n",
    "            - avg_loss: Average validation loss over all batches\n",
    "            - val_acc: Validation accuracy (computed using torchmetrics)\n",
    "            - val_f1: Validation F1 score (computed using torchmetrics)\n",
    "            - precision: Validation precision (computed using torchmetrics)\n",
    "            - recall: Validation recall (computed using torchmetrics)\n",
    "            - cm: Confusion matrix as numpy array, shape (2, 2)\n",
    "            - preds: Numpy array of class predictions {0, 1}\n",
    "            - targets: Numpy array of ground truth labels {0, 1}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    logit_list, label_list = [], []\n",
    "\n",
    "    # Introspect model signature once\n",
    "    sig = inspect.signature(model.forward)\n",
    "    param_names = set(sig.parameters.keys())\n",
    "    use_x          = ('x'          in param_names)\n",
    "    use_edge_index = ('edge_index' in param_names)\n",
    "    use_edge_attr  = ('edge_attr'  in param_names)\n",
    "    use_batch      = ('batch'      in param_names)\n",
    "\n",
    "    # Create metrics for validation\n",
    "    val_accuracy_metric = torchmetrics.Accuracy(task='binary').to(device)\n",
    "    val_f1_metric = torchmetrics.F1Score(task='binary').to(device)\n",
    "\n",
    "    # Determine total batches for logging\n",
    "    total_batches = len(val_loader)\n",
    "    epoch_str = f\"{epoch}/{n_epochs}\" if (epoch is not None and n_epochs is not None) else \"1/1\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass - build kwargs based on model signature\n",
    "            kwargs = {}\n",
    "            if use_x:          kwargs['x'] = batch.x\n",
    "            if use_edge_index: kwargs['edge_index'] = batch.edge_index\n",
    "            if use_edge_attr and hasattr(batch, 'edge_attr'):\n",
    "                kwargs['edge_attr'] = batch.edge_attr\n",
    "            if use_batch and hasattr(batch, 'batch'):\n",
    "                kwargs['batch'] = batch.batch\n",
    "            logits = model(**kwargs)\n",
    "\n",
    "            y_gt = batch.y.view(-1).long()\n",
    "\n",
    "            # Compute loss\n",
    "            batch_loss = loss_fn(logits, y_gt).item()\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            # Get predictions and update metrics\n",
    "            batch_preds = torch.argmax(logits, dim=-1)\n",
    "            val_accuracy_metric.update(batch_preds, y_gt)\n",
    "            val_f1_metric.update(batch_preds, y_gt)\n",
    "\n",
    "            logit_list.append(batch_preds.detach().cpu().numpy().reshape(-1))\n",
    "            label_list.append(y_gt.detach().cpu().numpy().reshape(-1))\n",
    "\n",
    "            # Log per-batch progress if log_final_only=False\n",
    "            if verbose and not log_final_only:\n",
    "                current_batch = batch_idx + 1\n",
    "                current_loss = total_loss / current_batch\n",
    "                current_acc = val_accuracy_metric.compute().item()\n",
    "                current_f1 = val_f1_metric.compute().item()\n",
    "                print(f\"Epoch [{epoch_str}], Step [{current_batch}/{total_batches}], \"\n",
    "                      f\"Loss: {current_loss:.4f}, Acc: {current_acc:.4f}, F1: {current_f1:.4f}\")\n",
    "\n",
    "    # Compute final metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    val_acc = val_accuracy_metric.compute().item()\n",
    "    val_f1 = val_f1_metric.compute().item()\n",
    "    preds = np.hstack(logit_list).ravel()\n",
    "    targets = np.hstack(label_list).ravel()\n",
    "\n",
    "    # Compute precision, recall, and confusion matrix\n",
    "    preds_tensor = torch.from_numpy(preds).long().to(device)\n",
    "    targets_tensor = torch.from_numpy(targets).long().to(device)\n",
    "\n",
    "    precision_metric = torchmetrics.Precision(task='binary').to(device)\n",
    "    recall_metric = torchmetrics.Recall(task='binary').to(device)\n",
    "\n",
    "    precision = precision_metric(preds_tensor, targets_tensor).item()\n",
    "    recall = recall_metric(preds_tensor, targets_tensor).item()\n",
    "    cm = confusion_matrix(targets, preds, labels=[0, 1])\n",
    "\n",
    "    # Log final results only once (or only time if log_final_only=True)\n",
    "    if verbose and log_final_only:\n",
    "        print(f\"Epoch [{epoch_str}], Step [{total_batches}/{total_batches}], \"\n",
    "              f\"Loss: {avg_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    return avg_loss, val_acc, val_f1, precision, recall, cm, preds, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad29721",
   "metadata": {},
   "source": [
    "### 3.3 K-Fold Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5816c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kfold_training(\n",
    "    *,\n",
    "    fold_loaders,\n",
    "    model_fn,\n",
    "    optimizer_fn,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    epochs: int,\n",
    "    experiment_name: str,\n",
    "    scheduler_fn=None,\n",
    "    patience=10,\n",
    "    min_delta=0.0001,\n",
    "    early_stop_metric='loss'\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs K-fold cross-validation training without using MetricsLogger.\n",
    "\n",
    "    All metric tracking is done explicitly with lists, making the code more\n",
    "    readable and eliminating ambiguity.\n",
    "\n",
    "    Args:\n",
    "        fold_loaders: List of (train_loader, val_loader) pairs from get_kfold_subject_loaders_v2\n",
    "        model_fn: Callable that returns a new model instance\n",
    "        optimizer_fn: Callable that takes model and returns optimizer\n",
    "        loss_fn: Loss function (e.g., nn.CrossEntropyLoss)\n",
    "        device: Device to train on ('cpu' or 'cuda')\n",
    "        epochs: Number of epochs per fold\n",
    "        experiment_name: Name for saving results\n",
    "        scheduler_fn: Optional callable that takes optimizer and returns scheduler\n",
    "        patience: Number of epochs to wait for improvement before stopping\n",
    "        min_delta: Minimum change to qualify as improvement\n",
    "        early_stop_metric: Metric to monitor ('loss', 'f1', 'accuracy')\n",
    "    Returns:\n",
    "        dict: K-fold training results and metrics\n",
    "\n",
    "    \"\"\"\n",
    "    exp_dir = get_experiment_dir(experiment_name)\n",
    "    all_fold_metrics = []\n",
    "    final_cms = []\n",
    "    best_epochs_per_fold = []\n",
    "\n",
    "    for fold_idx, (train_loader, val_loader) in enumerate(fold_loaders, start=1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Fold {fold_idx}/{len(fold_loaders)}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Initialize model, optimizer, scheduler\n",
    "        model = model_fn().to(device)\n",
    "        optimizer = optimizer_fn(model)\n",
    "        scheduler = scheduler_fn(optimizer) if scheduler_fn else None\n",
    "\n",
    "        # Initialize tracking lists\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        train_f1s = []\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        val_f1s = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        confusion_matrices = []\n",
    "\n",
    "        # Track best model for this fold based on early_stop_metric\n",
    "        if early_stop_metric == 'loss':\n",
    "            best_metric_value = float('inf')\n",
    "        elif early_stop_metric in ['f1', 'accuracy']:\n",
    "            best_metric_value = 0.0\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid early_stop_metric: {early_stop_metric}. Must be 'loss', 'f1', or 'accuracy'\")\n",
    "\n",
    "        best_model_state = None\n",
    "        best_epoch = 0\n",
    "\n",
    "        # NEW: Initialize early stopping for this fold\n",
    "        mode = 'min' if early_stop_metric == 'loss' else 'max'\n",
    "        early_stopper = EarlyStopping(patience=patience, min_delta=min_delta,\n",
    "                                      mode=mode, verbose=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            tr_loss, tr_acc, tr_f1 = train(\n",
    "                model, train_loader, optimizer, loss_fn, device,\n",
    "                epoch=epoch + 1, n_epochs=epochs, verbose=True, log_freq=10\n",
    "            )\n",
    "\n",
    "            # Validation phase\n",
    "            vl_loss, vl_acc, vl_f1, precision, recall, cm, vl_preds, vl_targets = val(\n",
    "                model, val_loader, loss_fn, device,\n",
    "                epoch=epoch + 1, n_epochs=epochs, verbose=False, log_final_only=True\n",
    "            )\n",
    "\n",
    "            # Step scheduler\n",
    "            if scheduler:\n",
    "                scheduler.step(vl_f1)\n",
    "\n",
    "            # Append metrics\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            train_f1s.append(tr_f1)\n",
    "            val_losses.append(vl_loss)\n",
    "            val_accs.append(vl_acc)\n",
    "            val_f1s.append(vl_f1)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            confusion_matrices.append(cm.tolist())\n",
    "\n",
    "            # Track best model by early_stop_metric (consistent with early stopping)\n",
    "            current_metric = {'loss': vl_loss, 'accuracy': vl_acc, 'f1': vl_f1}[early_stop_metric]\n",
    "\n",
    "            is_better = False\n",
    "            if early_stop_metric == 'loss':\n",
    "                is_better = current_metric < best_metric_value\n",
    "            else:  # f1 or accuracy\n",
    "                is_better = current_metric > best_metric_value\n",
    "\n",
    "            if is_better:\n",
    "                best_metric_value = current_metric\n",
    "                best_model_state = model.state_dict()\n",
    "                best_epoch = epoch\n",
    "                metric_name = {'loss': 'Loss', 'accuracy': 'Accuracy', 'f1': 'F1'}[early_stop_metric]\n",
    "                print(f\"  â˜… New best model! Val {metric_name}: {current_metric:.6f}\")\n",
    "\n",
    "            # Print summary\n",
    "            print(\n",
    "                f\"[Fold {fold_idx}] Epoch {epoch+1}/{epochs}  \"\n",
    "                f\"TR L:{tr_loss:.4f} Acc:{tr_acc:.4f} F1:{tr_f1:.4f} | \"\n",
    "                f\"VL L:{vl_loss:.4f} Acc:{vl_acc:.4f} F1:{vl_f1:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Check early stopping\n",
    "            stop_metric = {'loss': vl_loss, 'accuracy': vl_acc, 'f1': vl_f1}[early_stop_metric]\n",
    "            if early_stopper(stop_metric, epoch):\n",
    "                print(f\"\\n[Fold {fold_idx}] Early stopping at epoch {epoch+1}\")\n",
    "                print(f\"[Fold {fold_idx}] Best model was at epoch {best_epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Save per-fold metrics\n",
    "        fold_metrics = {\n",
    "            \"train_loss\": train_losses,\n",
    "            \"train_accuracy\": train_accs,\n",
    "            \"train_f1\": train_f1s,\n",
    "            \"val_loss\": val_losses,\n",
    "            \"val_accuracy\": val_accs,\n",
    "            \"val_f1\": val_f1s,\n",
    "            \"precision\": precisions,\n",
    "            \"recall\": recalls,\n",
    "            \"confusion_matrix\": confusion_matrices,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"stopped_at_epoch\": len(train_losses) - 1,\n",
    "        }\n",
    "        save_metrics(fold_metrics, exp_dir, f\"{experiment_name}_fold_{fold_idx}\")\n",
    "        all_fold_metrics.append(fold_metrics)\n",
    "        best_epochs_per_fold.append(best_epoch)\n",
    "\n",
    "        # Load and save best model\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            save_best_model(model, exp_dir, f\"{experiment_name}_fold_{fold_idx}\")\n",
    "\n",
    "        # FIXED: Use confusion matrix from BEST epoch, not last\n",
    "        final_cms.append(np.array(confusion_matrices[best_epoch]))\n",
    "\n",
    "        print(f\"\\n[Fold {fold_idx}] Summary:\")\n",
    "        print(f\"  Best epoch: {best_epoch+1}/{len(train_losses)}\")\n",
    "        print(f\"  Best val loss: {val_losses[best_epoch]:.4f}\")\n",
    "        print(f\"  Best val accuracy: {val_accs[best_epoch]:.4f}\")\n",
    "        print(f\"  Best val F1: {val_f1s[best_epoch]:.4f}\")\n",
    "\n",
    "    # Aggregate overall metrics\n",
    "    # For curves, pad shorter folds with NaN to handle different stopping points\n",
    "    max_epochs = max(len(m[\"train_loss\"]) for m in all_fold_metrics)\n",
    "\n",
    "    def pad_to_max(arr, max_len):\n",
    "        \"\"\"Pad array with NaN to max length\"\"\"\n",
    "        padded = np.full(max_len, np.nan)\n",
    "        padded[:len(arr)] = arr\n",
    "        return padded\n",
    "\n",
    "    tl = np.array([pad_to_max(m[\"train_loss\"], max_epochs) for m in all_fold_metrics])\n",
    "    ta = np.array([pad_to_max(m[\"train_accuracy\"], max_epochs) for m in all_fold_metrics])\n",
    "    tf = np.array([pad_to_max(m[\"train_f1\"], max_epochs) for m in all_fold_metrics])\n",
    "    vl = np.array([pad_to_max(m[\"val_loss\"], max_epochs) for m in all_fold_metrics])\n",
    "    va = np.array([pad_to_max(m[\"val_accuracy\"], max_epochs) for m in all_fold_metrics])\n",
    "    vf = np.array([pad_to_max(m[\"val_f1\"], max_epochs) for m in all_fold_metrics])\n",
    "\n",
    "    # Use nanmean to ignore NaN from early stopped folds\n",
    "    overall_metrics = {\n",
    "        \"train_loss\": np.nanmean(tl, axis=0).tolist(),\n",
    "        \"train_accuracy\": np.nanmean(ta, axis=0).tolist(),\n",
    "        \"train_f1\": np.nanmean(tf, axis=0).tolist(),\n",
    "        \"val_loss\": np.nanmean(vl, axis=0).tolist(),\n",
    "        \"val_accuracy\": np.nanmean(va, axis=0).tolist(),\n",
    "        \"val_f1\": np.nanmean(vf, axis=0).tolist(),\n",
    "        \"best_epochs_per_fold\": [e+1 for e in best_epochs_per_fold],\n",
    "    }\n",
    "\n",
    "    best_fold_precisions = [all_fold_metrics[i][\"precision\"][best_epochs_per_fold[i]]\n",
    "                            for i in range(len(all_fold_metrics))]\n",
    "    best_fold_recalls = [all_fold_metrics[i][\"recall\"][best_epochs_per_fold[i]]\n",
    "                        for i in range(len(all_fold_metrics))]\n",
    "\n",
    "    best_fold_accuracies = [all_fold_metrics[i][\"val_accuracy\"][best_epochs_per_fold[i]]\n",
    "                            for i in range(len(all_fold_metrics))]\n",
    "    best_fold_f1s = [all_fold_metrics[i][\"val_f1\"][best_epochs_per_fold[i]]\n",
    "                    for i in range(len(all_fold_metrics))]\n",
    "\n",
    "    # Aggregate confusion matrix (sum across folds)\n",
    "    agg_cm = np.sum(final_cms, axis=0) if final_cms else np.zeros((2, 2), int)\n",
    "\n",
    "    # Calculate overall metrics from aggregated CM\n",
    "    if agg_cm.shape == (2, 2) and agg_cm.sum() > 0:\n",
    "        tn, fp, fn, tp = agg_cm.ravel()\n",
    "        overall_accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0\n",
    "        overall_precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        overall_recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) \\\n",
    "                     if (overall_precision + overall_recall) > 0 else 0.0\n",
    "    else:\n",
    "        overall_precision = overall_recall = overall_f1 = 0.0\n",
    "\n",
    "    overall_metrics.update({\n",
    "        \"accuracy_mean\": np.mean(best_fold_accuracies),\n",
    "        \"f1_mean\": np.mean(best_fold_f1s),\n",
    "        \"precision_mean\": np.mean(best_fold_precisions),\n",
    "        \"recall_mean\": np.mean(best_fold_recalls),\n",
    "        \"accuracy_overall\": overall_accuracy,\n",
    "        \"precision_overall\": overall_precision,\n",
    "        \"recall_overall\": overall_recall,\n",
    "        \"f1_overall\": overall_f1,\n",
    "        \"confusion_matrix_overall\": agg_cm.tolist(),\n",
    "    })\n",
    "    save_metrics(overall_metrics, exp_dir, f\"{experiment_name}_kfold_overall\")\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"K-Fold Training Complete: {experiment_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Best epochs per fold: {overall_metrics['best_epochs_per_fold']}\")\n",
    "    print(f\"Overall Metrics (from best epochs across {len(fold_loaders)} folds):\")\n",
    "    print(f\"  Precision: {overall_precision:.4f}\")\n",
    "    print(f\"  Recall:    {overall_recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {overall_f1:.4f}\")\n",
    "    print(f\"  Confusion Matrix:\\n{agg_cm}\")\n",
    "\n",
    "    return {\"folds\": all_fold_metrics, \"overall\": overall_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f223ba",
   "metadata": {},
   "source": [
    "### 3.4 Execute 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bad5a8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Cross Entropy Loss\n",
      "\n",
      "Training Configuration:\n",
      "  Epochs: 50\n",
      "  Patience: 10\n",
      "  Early Stop Metric: loss\n",
      "  Experiment Name: flexible_gat_5fold\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "MIN_DELTA = 0.0001\n",
    "EARLY_STOP_METRIC = 'loss'  # 'loss', 'f1', or 'accuracy'\n",
    "EXPERIMENT_NAME = 'flexible_gat_5fold'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model factory function\n",
    "def create_model():\n",
    "    return FlexibleGATNet(**MODEL_CONFIG)\n",
    "\n",
    "# Define optimizer factory function\n",
    "def create_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Optional: Define scheduler factory function\n",
    "def create_scheduler(optimizer):\n",
    "    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=3, verbose=False)\n",
    "\n",
    "# Calculate class weights if enabled\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    class_weights = calculate_class_weights(fold_loaders_v2, device=DEVICE, fold_idx=0, use_sqrt=USE_SQRT_WEIGHTS)\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "else:\n",
    "    class_weights = None\n",
    "\n",
    "# Define loss function\n",
    "if USE_FOCAL_LOSS:\n",
    "    loss_fn = FocalLoss(alpha=class_weights, gamma=FOCAL_GAMMA)\n",
    "    print(f\"Using Focal Loss (gamma={FOCAL_GAMMA})\")\n",
    "else:\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    print(f\"Using Cross Entropy Loss\")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Patience: {PATIENCE}\")\n",
    "print(f\"  Early Stop Metric: {EARLY_STOP_METRIC}\")\n",
    "print(f\"  Experiment Name: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bac3b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Fold 1/5\n",
      "======================================================================\n",
      "Epoch [1/50], Step [10/11], Loss: 0.7273, Acc: 0.6125, F1: 0.5373\n",
      "Epoch [1/50], Step [11/11], Loss: 0.7573, Acc: 0.5952, F1: 0.5278\n",
      "  â˜… New best model! Val Loss: 0.472359\n",
      "[Fold 1] Epoch 1/50  TR L:0.7573 Acc:0.5952 F1:0.5278 | VL L:0.4724 Acc:0.8182 F1:0.7143\n",
      "Epoch [2/50], Step [10/11], Loss: 0.5213, Acc: 0.7875, F1: 0.7119\n",
      "Epoch [2/50], Step [11/11], Loss: 0.5076, Acc: 0.7976, F1: 0.7213\n",
      "  â˜… New best model! Val Loss: 0.428159\n",
      "[Fold 1] Epoch 2/50  TR L:0.5076 Acc:0.7976 F1:0.7213 | VL L:0.4282 Acc:0.7727 F1:0.6667\n",
      "Epoch [3/50], Step [10/11], Loss: 0.6228, Acc: 0.6500, F1: 0.4615\n",
      "Epoch [3/50], Step [11/11], Loss: 0.6985, Acc: 0.6429, F1: 0.4828\n",
      "  â˜… New best model! Val Loss: 0.402076\n",
      "[Fold 1] Epoch 3/50  TR L:0.6985 Acc:0.6429 F1:0.4828 | VL L:0.4021 Acc:0.8182 F1:0.7778\n",
      "Epoch [4/50], Step [10/11], Loss: 0.7794, Acc: 0.5875, F1: 0.4762\n",
      "Epoch [4/50], Step [11/11], Loss: 0.7602, Acc: 0.5833, F1: 0.4615\n",
      "[Fold 1] Epoch 4/50  TR L:0.7602 Acc:0.5833 F1:0.4615 | VL L:0.5040 Acc:0.7273 F1:0.6667\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [5/50], Step [10/11], Loss: 0.5634, Acc: 0.7250, F1: 0.6207\n",
      "Epoch [5/50], Step [11/11], Loss: 0.5546, Acc: 0.7262, F1: 0.6230\n",
      "[Fold 1] Epoch 5/50  TR L:0.5546 Acc:0.7262 F1:0.6230 | VL L:0.5327 Acc:0.6364 F1:0.5556\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [6/50], Step [10/11], Loss: 0.5576, Acc: 0.6250, F1: 0.5312\n",
      "Epoch [6/50], Step [11/11], Loss: 0.5511, Acc: 0.6310, F1: 0.5373\n",
      "[Fold 1] Epoch 6/50  TR L:0.5511 Acc:0.6310 F1:0.5373 | VL L:0.4338 Acc:0.8182 F1:0.8000\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [7/50], Step [10/11], Loss: 0.6106, Acc: 0.6625, F1: 0.5263\n",
      "Epoch [7/50], Step [11/11], Loss: 0.5862, Acc: 0.6786, F1: 0.5263\n",
      "[Fold 1] Epoch 7/50  TR L:0.5862 Acc:0.6786 F1:0.5263 | VL L:0.5711 Acc:0.6818 F1:0.5882\n",
      "  EarlyStopping: 4/10 epochs without improvement\n",
      "Epoch [8/50], Step [10/11], Loss: 0.6263, Acc: 0.6875, F1: 0.5902\n",
      "Epoch [8/50], Step [11/11], Loss: 0.5890, Acc: 0.7024, F1: 0.6154\n",
      "  â˜… New best model! Val Loss: 0.369244\n",
      "[Fold 1] Epoch 8/50  TR L:0.5890 Acc:0.7024 F1:0.6154 | VL L:0.3692 Acc:0.8182 F1:0.7143\n",
      "Epoch [9/50], Step [10/11], Loss: 0.5398, Acc: 0.7625, F1: 0.6885\n",
      "Epoch [9/50], Step [11/11], Loss: 0.5514, Acc: 0.7619, F1: 0.6875\n",
      "  â˜… New best model! Val Loss: 0.348781\n",
      "[Fold 1] Epoch 9/50  TR L:0.5514 Acc:0.7619 F1:0.6875 | VL L:0.3488 Acc:0.8182 F1:0.7778\n",
      "Epoch [10/50], Step [10/11], Loss: 0.5185, Acc: 0.7375, F1: 0.6667\n",
      "Epoch [10/50], Step [11/11], Loss: 0.5352, Acc: 0.7262, F1: 0.6462\n",
      "  â˜… New best model! Val Loss: 0.315685\n",
      "[Fold 1] Epoch 10/50  TR L:0.5352 Acc:0.7262 F1:0.6462 | VL L:0.3157 Acc:0.9091 F1:0.8889\n",
      "Epoch [11/50], Step [10/11], Loss: 0.5550, Acc: 0.7250, F1: 0.6452\n",
      "Epoch [11/50], Step [11/11], Loss: 0.5686, Acc: 0.7262, F1: 0.6462\n",
      "[Fold 1] Epoch 11/50  TR L:0.5686 Acc:0.7262 F1:0.6462 | VL L:0.4404 Acc:0.8182 F1:0.7500\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [12/50], Step [10/11], Loss: 0.4426, Acc: 0.8375, F1: 0.7797\n",
      "Epoch [12/50], Step [11/11], Loss: 0.4384, Acc: 0.8333, F1: 0.7742\n",
      "[Fold 1] Epoch 12/50  TR L:0.4384 Acc:0.8333 F1:0.7742 | VL L:0.4652 Acc:0.7273 F1:0.5714\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [13/50], Step [10/11], Loss: 0.4741, Acc: 0.7750, F1: 0.7097\n",
      "Epoch [13/50], Step [11/11], Loss: 0.5254, Acc: 0.7619, F1: 0.6970\n",
      "[Fold 1] Epoch 13/50  TR L:0.5254 Acc:0.7619 F1:0.6970 | VL L:0.3783 Acc:0.8636 F1:0.8421\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [14/50], Step [10/11], Loss: 0.4885, Acc: 0.7250, F1: 0.6333\n",
      "Epoch [14/50], Step [11/11], Loss: 0.4980, Acc: 0.7262, F1: 0.6462\n",
      "[Fold 1] Epoch 14/50  TR L:0.4980 Acc:0.7262 F1:0.6462 | VL L:0.5495 Acc:0.7273 F1:0.6250\n",
      "  EarlyStopping: 4/10 epochs without improvement\n",
      "Epoch [15/50], Step [10/11], Loss: 0.5001, Acc: 0.7375, F1: 0.6866\n",
      "Epoch [15/50], Step [11/11], Loss: 0.4645, Acc: 0.7500, F1: 0.6957\n",
      "[Fold 1] Epoch 15/50  TR L:0.4645 Acc:0.7500 F1:0.6957 | VL L:0.4877 Acc:0.6818 F1:0.6316\n",
      "  EarlyStopping: 5/10 epochs without improvement\n",
      "Epoch [16/50], Step [10/11], Loss: 0.4782, Acc: 0.7750, F1: 0.6667\n",
      "Epoch [16/50], Step [11/11], Loss: 0.4949, Acc: 0.7619, F1: 0.6552\n",
      "[Fold 1] Epoch 16/50  TR L:0.4949 Acc:0.7619 F1:0.6552 | VL L:0.4486 Acc:0.7273 F1:0.6667\n",
      "  EarlyStopping: 6/10 epochs without improvement\n",
      "Epoch [17/50], Step [10/11], Loss: 0.4582, Acc: 0.8375, F1: 0.7797\n",
      "Epoch [17/50], Step [11/11], Loss: 0.4506, Acc: 0.8333, F1: 0.7742\n",
      "[Fold 1] Epoch 17/50  TR L:0.4506 Acc:0.8333 F1:0.7742 | VL L:0.4801 Acc:0.7273 F1:0.7273\n",
      "  EarlyStopping: 7/10 epochs without improvement\n",
      "Epoch [18/50], Step [10/11], Loss: 0.5626, Acc: 0.7250, F1: 0.6765\n",
      "Epoch [18/50], Step [11/11], Loss: 0.5510, Acc: 0.7381, F1: 0.6765\n",
      "[Fold 1] Epoch 18/50  TR L:0.5510 Acc:0.7381 F1:0.6765 | VL L:0.4049 Acc:0.8182 F1:0.8000\n",
      "  EarlyStopping: 8/10 epochs without improvement\n",
      "Epoch [19/50], Step [10/11], Loss: 0.4001, Acc: 0.8500, F1: 0.8125\n",
      "Epoch [19/50], Step [11/11], Loss: 0.4300, Acc: 0.8452, F1: 0.8000\n",
      "[Fold 1] Epoch 19/50  TR L:0.4300 Acc:0.8452 F1:0.8000 | VL L:0.4575 Acc:0.7727 F1:0.7368\n",
      "  EarlyStopping: 9/10 epochs without improvement\n",
      "Epoch [20/50], Step [10/11], Loss: 0.4470, Acc: 0.8000, F1: 0.7576\n",
      "Epoch [20/50], Step [11/11], Loss: 0.4521, Acc: 0.7976, F1: 0.7463\n",
      "[Fold 1] Epoch 20/50  TR L:0.4521 Acc:0.7976 F1:0.7463 | VL L:0.4321 Acc:0.7273 F1:0.7273\n",
      "  EarlyStopping: 10/10 epochs without improvement\n",
      "  EarlyStopping: Stopping training at epoch 19\n",
      "  Best score: 0.315685 at epoch 9\n",
      "\n",
      "[Fold 1] Early stopping at epoch 20\n",
      "[Fold 1] Best model was at epoch 10\n",
      "â€¢ Saved best model weights to experiments/20251219/flexible_gat_5fold/flexible_gat_5fold_fold_1_best.pt\n",
      "\n",
      "[Fold 1] Summary:\n",
      "  Best epoch: 10/20\n",
      "  Best val loss: 0.3157\n",
      "  Best val accuracy: 0.9091\n",
      "  Best val F1: 0.8889\n",
      "\n",
      "======================================================================\n",
      "Fold 2/5\n",
      "======================================================================\n",
      "Epoch [1/50], Step [10/11], Loss: 0.7435, Acc: 0.6625, F1: 0.5424\n",
      "Epoch [1/50], Step [11/11], Loss: 0.7085, Acc: 0.6786, F1: 0.5574\n",
      "  â˜… New best model! Val Loss: 1.060052\n",
      "[Fold 2] Epoch 1/50  TR L:0.7085 Acc:0.6786 F1:0.5574 | VL L:1.0601 Acc:0.6818 F1:0.2222\n",
      "Epoch [2/50], Step [10/11], Loss: 0.7012, Acc: 0.6625, F1: 0.5846\n",
      "Epoch [2/50], Step [11/11], Loss: 0.6856, Acc: 0.6667, F1: 0.5882\n",
      "  â˜… New best model! Val Loss: 0.830147\n",
      "[Fold 2] Epoch 2/50  TR L:0.6856 Acc:0.6667 F1:0.5882 | VL L:0.8301 Acc:0.5455 F1:0.2857\n",
      "Epoch [3/50], Step [10/11], Loss: 0.6500, Acc: 0.7125, F1: 0.6102\n",
      "Epoch [3/50], Step [11/11], Loss: 0.6376, Acc: 0.7143, F1: 0.6129\n",
      "  â˜… New best model! Val Loss: 0.779057\n",
      "[Fold 2] Epoch 3/50  TR L:0.6376 Acc:0.7143 F1:0.6129 | VL L:0.7791 Acc:0.6818 F1:0.5882\n",
      "Epoch [4/50], Step [10/11], Loss: 0.7557, Acc: 0.6000, F1: 0.5000\n",
      "Epoch [4/50], Step [11/11], Loss: 0.7334, Acc: 0.6071, F1: 0.4923\n",
      "  â˜… New best model! Val Loss: 0.730986\n",
      "[Fold 2] Epoch 4/50  TR L:0.7334 Acc:0.6071 F1:0.4923 | VL L:0.7310 Acc:0.5000 F1:0.2667\n",
      "Epoch [5/50], Step [10/11], Loss: 0.5645, Acc: 0.6500, F1: 0.4815\n",
      "Epoch [5/50], Step [11/11], Loss: 0.5728, Acc: 0.6429, F1: 0.4828\n",
      "[Fold 2] Epoch 5/50  TR L:0.5728 Acc:0.6429 F1:0.4828 | VL L:0.8391 Acc:0.5455 F1:0.2857\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [6/50], Step [10/11], Loss: 0.5328, Acc: 0.6750, F1: 0.5667\n",
      "Epoch [6/50], Step [11/11], Loss: 0.5165, Acc: 0.6786, F1: 0.5714\n",
      "[Fold 2] Epoch 6/50  TR L:0.5165 Acc:0.6786 F1:0.5714 | VL L:0.7977 Acc:0.6818 F1:0.5333\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [7/50], Step [10/11], Loss: 0.5795, Acc: 0.7250, F1: 0.6667\n",
      "Epoch [7/50], Step [11/11], Loss: 0.5715, Acc: 0.7143, F1: 0.6471\n",
      "[Fold 2] Epoch 7/50  TR L:0.5715 Acc:0.7143 F1:0.6471 | VL L:0.9471 Acc:0.5455 F1:0.3750\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [8/50], Step [10/11], Loss: 0.5536, Acc: 0.7125, F1: 0.6102\n",
      "Epoch [8/50], Step [11/11], Loss: 0.5856, Acc: 0.7024, F1: 0.6032\n",
      "[Fold 2] Epoch 8/50  TR L:0.5856 Acc:0.7024 F1:0.6032 | VL L:0.9476 Acc:0.6364 F1:0.3333\n",
      "  EarlyStopping: 4/10 epochs without improvement\n",
      "Epoch [9/50], Step [10/11], Loss: 0.4502, Acc: 0.8000, F1: 0.7419\n",
      "Epoch [9/50], Step [11/11], Loss: 0.4487, Acc: 0.7976, F1: 0.7302\n",
      "[Fold 2] Epoch 9/50  TR L:0.4487 Acc:0.7976 F1:0.7302 | VL L:0.8790 Acc:0.5455 F1:0.1667\n",
      "  EarlyStopping: 5/10 epochs without improvement\n",
      "Epoch [10/50], Step [10/11], Loss: 0.4461, Acc: 0.8125, F1: 0.7541\n",
      "Epoch [10/50], Step [11/11], Loss: 0.4504, Acc: 0.8095, F1: 0.7500\n",
      "[Fold 2] Epoch 10/50  TR L:0.4504 Acc:0.8095 F1:0.7500 | VL L:0.9204 Acc:0.5909 F1:0.1818\n",
      "  EarlyStopping: 6/10 epochs without improvement\n",
      "Epoch [11/50], Step [10/11], Loss: 0.4669, Acc: 0.7875, F1: 0.7119\n",
      "Epoch [11/50], Step [11/11], Loss: 0.4405, Acc: 0.7976, F1: 0.7213\n",
      "[Fold 2] Epoch 11/50  TR L:0.4405 Acc:0.7976 F1:0.7213 | VL L:0.9338 Acc:0.5000 F1:0.1538\n",
      "  EarlyStopping: 7/10 epochs without improvement\n",
      "Epoch [12/50], Step [10/11], Loss: 0.4239, Acc: 0.8125, F1: 0.7692\n",
      "Epoch [12/50], Step [11/11], Loss: 0.4150, Acc: 0.8095, F1: 0.7647\n",
      "[Fold 2] Epoch 12/50  TR L:0.4150 Acc:0.8095 F1:0.7647 | VL L:0.9936 Acc:0.5455 F1:0.1667\n",
      "  EarlyStopping: 8/10 epochs without improvement\n",
      "Epoch [13/50], Step [10/11], Loss: 0.3717, Acc: 0.8500, F1: 0.8000\n",
      "Epoch [13/50], Step [11/11], Loss: 0.3519, Acc: 0.8571, F1: 0.8125\n",
      "[Fold 2] Epoch 13/50  TR L:0.3519 Acc:0.8571 F1:0.8125 | VL L:0.9895 Acc:0.5000 F1:0.1538\n",
      "  EarlyStopping: 9/10 epochs without improvement\n",
      "Epoch [14/50], Step [10/11], Loss: 0.3834, Acc: 0.8125, F1: 0.7692\n",
      "Epoch [14/50], Step [11/11], Loss: 0.3878, Acc: 0.8095, F1: 0.7576\n",
      "[Fold 2] Epoch 14/50  TR L:0.3878 Acc:0.8095 F1:0.7576 | VL L:1.0596 Acc:0.5455 F1:0.2857\n",
      "  EarlyStopping: 10/10 epochs without improvement\n",
      "  EarlyStopping: Stopping training at epoch 13\n",
      "  Best score: 0.730986 at epoch 3\n",
      "\n",
      "[Fold 2] Early stopping at epoch 14\n",
      "[Fold 2] Best model was at epoch 4\n",
      "â€¢ Saved best model weights to experiments/20251219/flexible_gat_5fold/flexible_gat_5fold_fold_2_best.pt\n",
      "\n",
      "[Fold 2] Summary:\n",
      "  Best epoch: 4/14\n",
      "  Best val loss: 0.7310\n",
      "  Best val accuracy: 0.5000\n",
      "  Best val F1: 0.2667\n",
      "\n",
      "======================================================================\n",
      "Fold 3/5\n",
      "======================================================================\n",
      "Epoch [1/50], Step [10/11], Loss: 0.7306, Acc: 0.5875, F1: 0.5075\n",
      "Epoch [1/50], Step [11/11], Loss: 0.7191, Acc: 0.5952, F1: 0.5143\n",
      "  â˜… New best model! Val Loss: 0.786350\n",
      "[Fold 3] Epoch 1/50  TR L:0.7191 Acc:0.5952 F1:0.5143 | VL L:0.7863 Acc:0.4091 F1:0.1333\n",
      "Epoch [2/50], Step [10/11], Loss: 0.7076, Acc: 0.6750, F1: 0.5000\n",
      "Epoch [2/50], Step [11/11], Loss: 0.6813, Acc: 0.6786, F1: 0.5091\n",
      "[Fold 3] Epoch 2/50  TR L:0.6813 Acc:0.6786 F1:0.5091 | VL L:0.8117 Acc:0.5000 F1:0.4211\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [3/50], Step [10/11], Loss: 0.4782, Acc: 0.7875, F1: 0.7213\n",
      "Epoch [3/50], Step [11/11], Loss: 0.5266, Acc: 0.7619, F1: 0.6875\n",
      "[Fold 3] Epoch 3/50  TR L:0.5266 Acc:0.7619 F1:0.6875 | VL L:0.8301 Acc:0.5000 F1:0.4762\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [4/50], Step [10/11], Loss: 0.4719, Acc: 0.7750, F1: 0.7097\n",
      "Epoch [4/50], Step [11/11], Loss: 0.4687, Acc: 0.7738, F1: 0.7077\n",
      "[Fold 3] Epoch 4/50  TR L:0.4687 Acc:0.7738 F1:0.7077 | VL L:0.9530 Acc:0.4545 F1:0.4545\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [5/50], Step [10/11], Loss: 0.4729, Acc: 0.7750, F1: 0.7000\n",
      "Epoch [5/50], Step [11/11], Loss: 0.4618, Acc: 0.7738, F1: 0.7077\n",
      "[Fold 3] Epoch 5/50  TR L:0.4618 Acc:0.7738 F1:0.7077 | VL L:0.8908 Acc:0.5909 F1:0.6087\n",
      "  EarlyStopping: 4/10 epochs without improvement\n",
      "Epoch [6/50], Step [10/11], Loss: 0.5248, Acc: 0.7125, F1: 0.6761\n",
      "Epoch [6/50], Step [11/11], Loss: 0.5645, Acc: 0.7024, F1: 0.6575\n",
      "[Fold 3] Epoch 6/50  TR L:0.5645 Acc:0.7024 F1:0.6575 | VL L:1.0032 Acc:0.5000 F1:0.5217\n",
      "  EarlyStopping: 5/10 epochs without improvement\n",
      "Epoch [7/50], Step [10/11], Loss: 0.6639, Acc: 0.7000, F1: 0.5385\n",
      "Epoch [7/50], Step [11/11], Loss: 0.6282, Acc: 0.7143, F1: 0.5714\n",
      "  â˜… New best model! Val Loss: 0.785249\n",
      "[Fold 3] Epoch 7/50  TR L:0.6282 Acc:0.7143 F1:0.5714 | VL L:0.7852 Acc:0.5000 F1:0.5217\n",
      "Epoch [8/50], Step [10/11], Loss: 0.4903, Acc: 0.7375, F1: 0.6557\n",
      "Epoch [8/50], Step [11/11], Loss: 0.5493, Acc: 0.7143, F1: 0.6364\n",
      "[Fold 3] Epoch 8/50  TR L:0.5493 Acc:0.7143 F1:0.6364 | VL L:1.1524 Acc:0.5000 F1:0.5217\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [9/50], Step [10/11], Loss: 0.5329, Acc: 0.7125, F1: 0.6349\n",
      "Epoch [9/50], Step [11/11], Loss: 0.5413, Acc: 0.7024, F1: 0.6154\n",
      "[Fold 3] Epoch 9/50  TR L:0.5413 Acc:0.7024 F1:0.6154 | VL L:1.1339 Acc:0.5455 F1:0.5833\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [10/50], Step [10/11], Loss: 0.4849, Acc: 0.7875, F1: 0.7536\n",
      "Epoch [10/50], Step [11/11], Loss: 0.5076, Acc: 0.7738, F1: 0.7324\n",
      "[Fold 3] Epoch 10/50  TR L:0.5076 Acc:0.7738 F1:0.7324 | VL L:1.0426 Acc:0.4545 F1:0.4545\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [11/50], Step [10/11], Loss: 0.5546, Acc: 0.7875, F1: 0.7018\n",
      "Epoch [11/50], Step [11/11], Loss: 0.5609, Acc: 0.7857, F1: 0.6897\n",
      "[Fold 3] Epoch 11/50  TR L:0.5609 Acc:0.7857 F1:0.6897 | VL L:1.0230 Acc:0.5000 F1:0.5217\n",
      "  EarlyStopping: 4/10 epochs without improvement\n",
      "Epoch [12/50], Step [10/11], Loss: 0.3497, Acc: 0.8500, F1: 0.8000\n",
      "Epoch [12/50], Step [11/11], Loss: 0.4131, Acc: 0.8452, F1: 0.7937\n",
      "[Fold 3] Epoch 12/50  TR L:0.4131 Acc:0.8452 F1:0.7937 | VL L:1.0151 Acc:0.5000 F1:0.4762\n",
      "  EarlyStopping: 5/10 epochs without improvement\n",
      "Epoch [13/50], Step [10/11], Loss: 0.4377, Acc: 0.7875, F1: 0.6909\n",
      "Epoch [13/50], Step [11/11], Loss: 0.4279, Acc: 0.7857, F1: 0.6786\n",
      "[Fold 3] Epoch 13/50  TR L:0.4279 Acc:0.7857 F1:0.6786 | VL L:1.1433 Acc:0.4545 F1:0.4545\n",
      "  EarlyStopping: 6/10 epochs without improvement\n",
      "Epoch [14/50], Step [10/11], Loss: 0.4132, Acc: 0.8125, F1: 0.7368\n",
      "Epoch [14/50], Step [11/11], Loss: 0.4318, Acc: 0.8095, F1: 0.7419\n",
      "[Fold 3] Epoch 14/50  TR L:0.4318 Acc:0.8095 F1:0.7419 | VL L:1.0516 Acc:0.4545 F1:0.4545\n",
      "  EarlyStopping: 7/10 epochs without improvement\n",
      "Epoch [15/50], Step [10/11], Loss: 0.4232, Acc: 0.8250, F1: 0.7667\n",
      "Epoch [15/50], Step [11/11], Loss: 0.4338, Acc: 0.8214, F1: 0.7619\n",
      "[Fold 3] Epoch 15/50  TR L:0.4338 Acc:0.8214 F1:0.7619 | VL L:1.0054 Acc:0.5000 F1:0.5217\n",
      "  EarlyStopping: 8/10 epochs without improvement\n",
      "Epoch [16/50], Step [10/11], Loss: 0.3982, Acc: 0.7875, F1: 0.7385\n",
      "Epoch [16/50], Step [11/11], Loss: 0.4186, Acc: 0.7857, F1: 0.7429\n",
      "[Fold 3] Epoch 16/50  TR L:0.4186 Acc:0.7857 F1:0.7429 | VL L:1.0582 Acc:0.4545 F1:0.4545\n",
      "  EarlyStopping: 9/10 epochs without improvement\n",
      "Epoch [17/50], Step [10/11], Loss: 0.4056, Acc: 0.7875, F1: 0.7385\n",
      "Epoch [17/50], Step [11/11], Loss: 0.4932, Acc: 0.7738, F1: 0.7164\n",
      "[Fold 3] Epoch 17/50  TR L:0.4932 Acc:0.7738 F1:0.7164 | VL L:1.0631 Acc:0.5000 F1:0.5217\n",
      "  EarlyStopping: 10/10 epochs without improvement\n",
      "  EarlyStopping: Stopping training at epoch 16\n",
      "  Best score: 0.785249 at epoch 6\n",
      "\n",
      "[Fold 3] Early stopping at epoch 17\n",
      "[Fold 3] Best model was at epoch 7\n",
      "â€¢ Saved best model weights to experiments/20251219/flexible_gat_5fold/flexible_gat_5fold_fold_3_best.pt\n",
      "\n",
      "[Fold 3] Summary:\n",
      "  Best epoch: 7/17\n",
      "  Best val loss: 0.7852\n",
      "  Best val accuracy: 0.5000\n",
      "  Best val F1: 0.5217\n",
      "\n",
      "======================================================================\n",
      "Fold 4/5\n",
      "======================================================================\n",
      "Epoch [1/50], Step [10/11], Loss: 0.7638, Acc: 0.5875, F1: 0.4762\n",
      "Epoch [1/50], Step [11/11], Loss: 0.7675, Acc: 0.5814, F1: 0.4857\n",
      "  â˜… New best model! Val Loss: 0.709279\n",
      "[Fold 4] Epoch 1/50  TR L:0.7675 Acc:0.5814 F1:0.4857 | VL L:0.7093 Acc:0.5500 F1:0.0000\n",
      "Epoch [2/50], Step [10/11], Loss: 0.7199, Acc: 0.6500, F1: 0.5000\n",
      "Epoch [2/50], Step [11/11], Loss: 0.7096, Acc: 0.6395, F1: 0.4918\n",
      "[Fold 4] Epoch 2/50  TR L:0.7096 Acc:0.6395 F1:0.4918 | VL L:0.7848 Acc:0.5000 F1:0.2857\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [3/50], Step [10/11], Loss: 0.6565, Acc: 0.6625, F1: 0.5091\n",
      "Epoch [3/50], Step [11/11], Loss: 0.7074, Acc: 0.6744, F1: 0.5484\n",
      "  â˜… New best model! Val Loss: 0.567920\n",
      "[Fold 4] Epoch 3/50  TR L:0.7074 Acc:0.6744 F1:0.5484 | VL L:0.5679 Acc:0.7000 F1:0.6667\n",
      "Epoch [4/50], Step [10/11], Loss: 0.5879, Acc: 0.6875, F1: 0.6154\n",
      "Epoch [4/50], Step [11/11], Loss: 0.5713, Acc: 0.6977, F1: 0.6389\n",
      "  â˜… New best model! Val Loss: 0.488117\n",
      "[Fold 4] Epoch 4/50  TR L:0.5713 Acc:0.6977 F1:0.6389 | VL L:0.4881 Acc:0.8000 F1:0.7778\n",
      "Epoch [5/50], Step [10/11], Loss: 0.5571, Acc: 0.6875, F1: 0.5763\n",
      "Epoch [5/50], Step [11/11], Loss: 0.5499, Acc: 0.6977, F1: 0.5806\n",
      "[Fold 4] Epoch 5/50  TR L:0.5499 Acc:0.6977 F1:0.5806 | VL L:0.5452 Acc:0.7000 F1:0.6250\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [6/50], Step [10/11], Loss: 0.5488, Acc: 0.7625, F1: 0.6885\n",
      "Epoch [6/50], Step [11/11], Loss: 0.5150, Acc: 0.7791, F1: 0.7164\n",
      "[Fold 4] Epoch 6/50  TR L:0.5150 Acc:0.7791 F1:0.7164 | VL L:0.5939 Acc:0.7000 F1:0.6250\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [7/50], Step [10/11], Loss: 0.5444, Acc: 0.7500, F1: 0.6552\n",
      "Epoch [7/50], Step [11/11], Loss: 0.5335, Acc: 0.7558, F1: 0.6667\n",
      "[Fold 4] Epoch 7/50  TR L:0.5335 Acc:0.7558 F1:0.6667 | VL L:0.5624 Acc:0.7000 F1:0.6250\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [8/50], Step [10/11], Loss: 0.5118, Acc: 0.7625, F1: 0.6780\n",
      "Epoch [8/50], Step [11/11], Loss: 0.5716, Acc: 0.7326, F1: 0.6349\n",
      "[Fold 4] Epoch 8/50  TR L:0.5716 Acc:0.7326 F1:0.6349 | VL L:0.5764 Acc:0.8000 F1:0.7143\n",
      "  EarlyStopping: 4/10 epochs without improvement\n",
      "Epoch [9/50], Step [10/11], Loss: 0.4369, Acc: 0.7750, F1: 0.6538\n",
      "Epoch [9/50], Step [11/11], Loss: 0.4516, Acc: 0.7674, F1: 0.6429\n",
      "[Fold 4] Epoch 9/50  TR L:0.4516 Acc:0.7674 F1:0.6429 | VL L:0.5164 Acc:0.8000 F1:0.7143\n",
      "  EarlyStopping: 5/10 epochs without improvement\n",
      "Epoch [10/50], Step [10/11], Loss: 0.5469, Acc: 0.7125, F1: 0.5660\n",
      "Epoch [10/50], Step [11/11], Loss: 0.5239, Acc: 0.7209, F1: 0.5862\n",
      "[Fold 4] Epoch 10/50  TR L:0.5239 Acc:0.7209 F1:0.5862 | VL L:0.5138 Acc:0.7500 F1:0.6667\n",
      "  EarlyStopping: 6/10 epochs without improvement\n",
      "Epoch [11/50], Step [10/11], Loss: 0.4062, Acc: 0.7875, F1: 0.7018\n",
      "Epoch [11/50], Step [11/11], Loss: 0.4706, Acc: 0.7442, F1: 0.6562\n",
      "[Fold 4] Epoch 11/50  TR L:0.4706 Acc:0.7442 F1:0.6562 | VL L:0.5631 Acc:0.7000 F1:0.5714\n",
      "  EarlyStopping: 7/10 epochs without improvement\n",
      "Epoch [12/50], Step [10/11], Loss: 0.4835, Acc: 0.7250, F1: 0.6071\n",
      "Epoch [12/50], Step [11/11], Loss: 0.4924, Acc: 0.7209, F1: 0.5862\n",
      "  â˜… New best model! Val Loss: 0.483318\n",
      "[Fold 4] Epoch 12/50  TR L:0.4924 Acc:0.7209 F1:0.5862 | VL L:0.4833 Acc:0.7500 F1:0.6154\n",
      "Epoch [13/50], Step [10/11], Loss: 0.4835, Acc: 0.8000, F1: 0.7333\n",
      "Epoch [13/50], Step [11/11], Loss: 0.5067, Acc: 0.8023, F1: 0.7302\n",
      "[Fold 4] Epoch 13/50  TR L:0.5067 Acc:0.8023 F1:0.7302 | VL L:0.4840 Acc:0.7500 F1:0.6154\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [14/50], Step [10/11], Loss: 0.4321, Acc: 0.7750, F1: 0.6897\n",
      "Epoch [14/50], Step [11/11], Loss: 0.4579, Acc: 0.7558, F1: 0.6557\n",
      "[Fold 4] Epoch 14/50  TR L:0.4579 Acc:0.7558 F1:0.6557 | VL L:0.5207 Acc:0.8000 F1:0.7500\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [15/50], Step [10/11], Loss: 0.3997, Acc: 0.8500, F1: 0.7931\n",
      "Epoch [15/50], Step [11/11], Loss: 0.3887, Acc: 0.8605, F1: 0.8065\n",
      "[Fold 4] Epoch 15/50  TR L:0.3887 Acc:0.8605 F1:0.8065 | VL L:0.5388 Acc:0.7000 F1:0.5714\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [16/50], Step [10/11], Loss: 0.3376, Acc: 0.8875, F1: 0.8525\n",
      "Epoch [16/50], Step [11/11], Loss: 0.3284, Acc: 0.8953, F1: 0.8657\n",
      "[Fold 4] Epoch 16/50  TR L:0.3284 Acc:0.8953 F1:0.8657 | VL L:0.5585 Acc:0.7500 F1:0.6667\n",
      "  EarlyStopping: 4/10 epochs without improvement\n",
      "Epoch [17/50], Step [10/11], Loss: 0.4719, Acc: 0.7750, F1: 0.7000\n",
      "Epoch [17/50], Step [11/11], Loss: 0.4450, Acc: 0.7907, F1: 0.7273\n",
      "[Fold 4] Epoch 17/50  TR L:0.4450 Acc:0.7907 F1:0.7273 | VL L:0.5657 Acc:0.7000 F1:0.5714\n",
      "  EarlyStopping: 5/10 epochs without improvement\n",
      "Epoch [18/50], Step [10/11], Loss: 0.4145, Acc: 0.8125, F1: 0.7541\n",
      "Epoch [18/50], Step [11/11], Loss: 0.4330, Acc: 0.8023, F1: 0.7463\n",
      "[Fold 4] Epoch 18/50  TR L:0.4330 Acc:0.8023 F1:0.7463 | VL L:0.5684 Acc:0.7500 F1:0.6154\n",
      "  EarlyStopping: 6/10 epochs without improvement\n",
      "Epoch [19/50], Step [10/11], Loss: 0.3712, Acc: 0.7875, F1: 0.7119\n",
      "Epoch [19/50], Step [11/11], Loss: 0.3569, Acc: 0.8023, F1: 0.7302\n",
      "[Fold 4] Epoch 19/50  TR L:0.3569 Acc:0.8023 F1:0.7302 | VL L:0.5495 Acc:0.7500 F1:0.6154\n",
      "  EarlyStopping: 7/10 epochs without improvement\n",
      "Epoch [20/50], Step [10/11], Loss: 0.3784, Acc: 0.8125, F1: 0.7458\n",
      "Epoch [20/50], Step [11/11], Loss: 0.3963, Acc: 0.8023, F1: 0.7385\n",
      "[Fold 4] Epoch 20/50  TR L:0.3963 Acc:0.8023 F1:0.7385 | VL L:0.6080 Acc:0.7500 F1:0.6154\n",
      "  EarlyStopping: 8/10 epochs without improvement\n",
      "Epoch [21/50], Step [10/11], Loss: 0.3239, Acc: 0.8625, F1: 0.8197\n",
      "Epoch [21/50], Step [11/11], Loss: 0.3700, Acc: 0.8605, F1: 0.8125\n",
      "[Fold 4] Epoch 21/50  TR L:0.3700 Acc:0.8605 F1:0.8125 | VL L:0.5583 Acc:0.7500 F1:0.6154\n",
      "  EarlyStopping: 9/10 epochs without improvement\n",
      "Epoch [22/50], Step [10/11], Loss: 0.3888, Acc: 0.8250, F1: 0.7812\n",
      "Epoch [22/50], Step [11/11], Loss: 0.3720, Acc: 0.8372, F1: 0.7941\n",
      "[Fold 4] Epoch 22/50  TR L:0.3720 Acc:0.8372 F1:0.7941 | VL L:0.5823 Acc:0.7000 F1:0.5714\n",
      "  EarlyStopping: 10/10 epochs without improvement\n",
      "  EarlyStopping: Stopping training at epoch 21\n",
      "  Best score: 0.483318 at epoch 11\n",
      "\n",
      "[Fold 4] Early stopping at epoch 22\n",
      "[Fold 4] Best model was at epoch 12\n",
      "â€¢ Saved best model weights to experiments/20251219/flexible_gat_5fold/flexible_gat_5fold_fold_4_best.pt\n",
      "\n",
      "[Fold 4] Summary:\n",
      "  Best epoch: 12/22\n",
      "  Best val loss: 0.4833\n",
      "  Best val accuracy: 0.7500\n",
      "  Best val F1: 0.6154\n",
      "\n",
      "======================================================================\n",
      "Fold 5/5\n",
      "======================================================================\n",
      "Epoch [1/50], Step [10/11], Loss: 0.6417, Acc: 0.6000, F1: 0.4074\n",
      "Epoch [1/50], Step [11/11], Loss: 0.6678, Acc: 0.6047, F1: 0.4138\n",
      "  â˜… New best model! Val Loss: 0.861809\n",
      "[Fold 5] Epoch 1/50  TR L:0.6678 Acc:0.6047 F1:0.4138 | VL L:0.8618 Acc:0.5000 F1:0.2857\n",
      "Epoch [2/50], Step [10/11], Loss: 0.6489, Acc: 0.6125, F1: 0.4561\n",
      "Epoch [2/50], Step [11/11], Loss: 0.6221, Acc: 0.6279, F1: 0.4667\n",
      "  â˜… New best model! Val Loss: 0.856684\n",
      "[Fold 5] Epoch 2/50  TR L:0.6221 Acc:0.6279 F1:0.4667 | VL L:0.8567 Acc:0.5000 F1:0.4444\n",
      "Epoch [3/50], Step [10/11], Loss: 0.5482, Acc: 0.7000, F1: 0.6364\n",
      "Epoch [3/50], Step [11/11], Loss: 0.5586, Acc: 0.6860, F1: 0.6197\n",
      "[Fold 5] Epoch 3/50  TR L:0.5586 Acc:0.6860 F1:0.6197 | VL L:0.9839 Acc:0.4500 F1:0.3529\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [4/50], Step [10/11], Loss: 0.5226, Acc: 0.7125, F1: 0.5965\n",
      "Epoch [4/50], Step [11/11], Loss: 0.4861, Acc: 0.7326, F1: 0.6349\n",
      "[Fold 5] Epoch 4/50  TR L:0.4861 Acc:0.7326 F1:0.6349 | VL L:0.8725 Acc:0.6000 F1:0.5000\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [5/50], Step [10/11], Loss: 0.4590, Acc: 0.7250, F1: 0.6071\n",
      "Epoch [5/50], Step [11/11], Loss: 0.4361, Acc: 0.7442, F1: 0.6452\n",
      "[Fold 5] Epoch 5/50  TR L:0.4361 Acc:0.7442 F1:0.6452 | VL L:1.1264 Acc:0.5500 F1:0.4000\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [6/50], Step [10/11], Loss: 0.5148, Acc: 0.7375, F1: 0.6441\n",
      "Epoch [6/50], Step [11/11], Loss: 0.5149, Acc: 0.7326, F1: 0.6462\n",
      "  â˜… New best model! Val Loss: 0.838317\n",
      "[Fold 5] Epoch 6/50  TR L:0.5149 Acc:0.7326 F1:0.6462 | VL L:0.8383 Acc:0.6000 F1:0.4286\n",
      "Epoch [7/50], Step [10/11], Loss: 0.5565, Acc: 0.7500, F1: 0.6429\n",
      "Epoch [7/50], Step [11/11], Loss: 0.5123, Acc: 0.7674, F1: 0.6667\n",
      "[Fold 5] Epoch 7/50  TR L:0.5123 Acc:0.7674 F1:0.6667 | VL L:1.1491 Acc:0.5500 F1:0.4706\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [8/50], Step [10/11], Loss: 0.5280, Acc: 0.7375, F1: 0.6182\n",
      "Epoch [8/50], Step [11/11], Loss: 0.5149, Acc: 0.7442, F1: 0.6333\n",
      "  â˜… New best model! Val Loss: 0.773302\n",
      "[Fold 5] Epoch 8/50  TR L:0.5149 Acc:0.7442 F1:0.6333 | VL L:0.7733 Acc:0.6500 F1:0.5882\n",
      "Epoch [9/50], Step [10/11], Loss: 0.4700, Acc: 0.7875, F1: 0.7213\n",
      "Epoch [9/50], Step [11/11], Loss: 0.4364, Acc: 0.8023, F1: 0.7385\n",
      "[Fold 5] Epoch 9/50  TR L:0.4364 Acc:0.8023 F1:0.7385 | VL L:1.1151 Acc:0.5000 F1:0.2857\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [10/50], Step [10/11], Loss: 0.4604, Acc: 0.7625, F1: 0.7077\n",
      "Epoch [10/50], Step [11/11], Loss: 0.4730, Acc: 0.7674, F1: 0.7059\n",
      "  â˜… New best model! Val Loss: 0.772296\n",
      "[Fold 5] Epoch 10/50  TR L:0.4730 Acc:0.7674 F1:0.7059 | VL L:0.7723 Acc:0.5500 F1:0.4706\n",
      "Epoch [11/50], Step [10/11], Loss: 0.3861, Acc: 0.8000, F1: 0.7500\n",
      "Epoch [11/50], Step [11/11], Loss: 0.3790, Acc: 0.8023, F1: 0.7385\n",
      "[Fold 5] Epoch 11/50  TR L:0.3790 Acc:0.8023 F1:0.7385 | VL L:1.0846 Acc:0.5000 F1:0.3750\n",
      "  EarlyStopping: 1/10 epochs without improvement\n",
      "Epoch [12/50], Step [10/11], Loss: 0.4777, Acc: 0.7500, F1: 0.6429\n",
      "Epoch [12/50], Step [11/11], Loss: 0.4772, Acc: 0.7442, F1: 0.6207\n",
      "[Fold 5] Epoch 12/50  TR L:0.4772 Acc:0.7442 F1:0.6207 | VL L:0.9479 Acc:0.6000 F1:0.4286\n",
      "  EarlyStopping: 2/10 epochs without improvement\n",
      "Epoch [13/50], Step [10/11], Loss: 0.4268, Acc: 0.7750, F1: 0.6897\n",
      "Epoch [13/50], Step [11/11], Loss: 0.4016, Acc: 0.7907, F1: 0.7000\n",
      "[Fold 5] Epoch 13/50  TR L:0.4016 Acc:0.7907 F1:0.7000 | VL L:0.9644 Acc:0.5000 F1:0.3750\n",
      "  EarlyStopping: 3/10 epochs without improvement\n",
      "Epoch [14/50], Step [10/11], Loss: 0.4256, Acc: 0.8125, F1: 0.7619\n",
      "Epoch [14/50], Step [11/11], Loss: 0.4147, Acc: 0.8256, F1: 0.7619\n",
      "[Fold 5] Epoch 14/50  TR L:0.4147 Acc:0.8256 F1:0.7619 | VL L:1.0492 Acc:0.5000 F1:0.4444\n",
      "  EarlyStopping: 4/10 epochs without improvement\n",
      "Epoch [15/50], Step [10/11], Loss: 0.3558, Acc: 0.8625, F1: 0.8308\n",
      "Epoch [15/50], Step [11/11], Loss: 0.3435, Acc: 0.8721, F1: 0.8358\n",
      "[Fold 5] Epoch 15/50  TR L:0.3435 Acc:0.8721 F1:0.8358 | VL L:0.8414 Acc:0.5500 F1:0.4706\n",
      "  EarlyStopping: 5/10 epochs without improvement\n",
      "Epoch [16/50], Step [10/11], Loss: 0.3371, Acc: 0.8375, F1: 0.7937\n",
      "Epoch [16/50], Step [11/11], Loss: 0.3427, Acc: 0.8372, F1: 0.7941\n",
      "[Fold 5] Epoch 16/50  TR L:0.3427 Acc:0.8372 F1:0.7941 | VL L:0.8674 Acc:0.5000 F1:0.3750\n",
      "  EarlyStopping: 6/10 epochs without improvement\n",
      "Epoch [17/50], Step [10/11], Loss: 0.3036, Acc: 0.8875, F1: 0.8364\n",
      "Epoch [17/50], Step [11/11], Loss: 0.2867, Acc: 0.8953, F1: 0.8525\n",
      "[Fold 5] Epoch 17/50  TR L:0.2867 Acc:0.8953 F1:0.8525 | VL L:0.9443 Acc:0.5000 F1:0.3750\n",
      "  EarlyStopping: 7/10 epochs without improvement\n",
      "Epoch [18/50], Step [10/11], Loss: 0.3444, Acc: 0.8125, F1: 0.7458\n",
      "Epoch [18/50], Step [11/11], Loss: 0.3640, Acc: 0.8140, F1: 0.7500\n",
      "[Fold 5] Epoch 18/50  TR L:0.3640 Acc:0.8140 F1:0.7500 | VL L:1.0319 Acc:0.5000 F1:0.3750\n",
      "  EarlyStopping: 8/10 epochs without improvement\n",
      "Epoch [19/50], Step [10/11], Loss: 0.3660, Acc: 0.8250, F1: 0.7742\n",
      "Epoch [19/50], Step [11/11], Loss: 0.3512, Acc: 0.8256, F1: 0.7692\n",
      "[Fold 5] Epoch 19/50  TR L:0.3512 Acc:0.8256 F1:0.7692 | VL L:0.8905 Acc:0.5500 F1:0.4000\n",
      "  EarlyStopping: 9/10 epochs without improvement\n",
      "Epoch [20/50], Step [10/11], Loss: 0.3470, Acc: 0.8375, F1: 0.7636\n",
      "Epoch [20/50], Step [11/11], Loss: 0.3632, Acc: 0.8256, F1: 0.7458\n",
      "[Fold 5] Epoch 20/50  TR L:0.3632 Acc:0.8256 F1:0.7458 | VL L:0.8724 Acc:0.5500 F1:0.4000\n",
      "  EarlyStopping: 10/10 epochs without improvement\n",
      "  EarlyStopping: Stopping training at epoch 19\n",
      "  Best score: 0.772296 at epoch 9\n",
      "\n",
      "[Fold 5] Early stopping at epoch 20\n",
      "[Fold 5] Best model was at epoch 10\n",
      "â€¢ Saved best model weights to experiments/20251219/flexible_gat_5fold/flexible_gat_5fold_fold_5_best.pt\n",
      "\n",
      "[Fold 5] Summary:\n",
      "  Best epoch: 10/20\n",
      "  Best val loss: 0.7723\n",
      "  Best val accuracy: 0.5500\n",
      "  Best val F1: 0.4706\n",
      "\n",
      "======================================================================\n",
      "K-Fold Training Complete: flexible_gat_5fold\n",
      "======================================================================\n",
      "Best epochs per fold: [10, 4, 7, 12, 10]\n",
      "Overall Metrics (from best epochs across 5 folds):\n",
      "  Precision: 0.5217\n",
      "  Recall:    0.6000\n",
      "  F1 Score:  0.5581\n",
      "  Confusion Matrix:\n",
      "[[44 22]\n",
      " [16 24]]\n"
     ]
    }
   ],
   "source": [
    "# Run 5-fold cross-validation training\n",
    "results = perform_kfold_training(\n",
    "    fold_loaders=fold_loaders_v2,\n",
    "    model_fn=create_model,\n",
    "    optimizer_fn=create_optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=DEVICE,\n",
    "    epochs=EPOCHS,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    scheduler_fn=create_scheduler,\n",
    "    patience=PATIENCE,\n",
    "    min_delta=MIN_DELTA,\n",
    "    early_stop_metric=EARLY_STOP_METRIC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fa965",
   "metadata": {},
   "source": [
    "### 3.5 Visualize and Report Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae337b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "K-FOLD CROSS-VALIDATION RESULTS\n",
      "================================================================================\n",
      "Experiment: flexible_gat_5fold\n",
      "Folder: ./experiments/20251219/flexible_gat_5fold\n",
      "Number of Folds: 5\n",
      "\n",
      "PER-FOLD BEST EPOCH PERFORMANCE\n",
      "+--------+------------+----------+--------+-----------+--------+\n",
      "| Fold   | Best Epoch | Accuracy |     F1 | Precision | Recall |\n",
      "+--------+------------+----------+--------+-----------+--------+\n",
      "| Fold 1 |         10 |   0.9091 | 0.8889 |    0.8000 | 1.0000 |\n",
      "| Fold 2 |          4 |   0.5000 | 0.2667 |    0.2857 | 0.2500 |\n",
      "| Fold 3 |          7 |   0.5000 | 0.5217 |    0.4000 | 0.7500 |\n",
      "| Fold 4 |         12 |   0.7500 | 0.6154 |    0.8000 | 0.5000 |\n",
      "| Fold 5 |         10 |   0.5500 | 0.4706 |    0.4444 | 0.5000 |\n",
      "+--------+------------+----------+--------+-----------+--------+\n",
      "\n",
      "PER-FOLD FINAL EPOCH PERFORMANCE\n",
      "+--------+----------+--------+-----------+--------+\n",
      "| Fold   | Accuracy |     F1 | Precision | Recall |\n",
      "+--------+----------+--------+-----------+--------+\n",
      "| Fold 1 |   0.7273 | 0.7273 |    0.5714 | 1.0000 |\n",
      "| Fold 2 |   0.5455 | 0.2857 |    0.3333 | 0.2500 |\n",
      "| Fold 3 |   0.5000 | 0.5217 |    0.4000 | 0.7500 |\n",
      "| Fold 4 |   0.7000 | 0.5714 |    0.6667 | 0.5000 |\n",
      "| Fold 5 |   0.5500 | 0.4000 |    0.4286 | 0.3750 |\n",
      "+--------+----------+--------+-----------+--------+\n",
      "\n",
      "OVERALL CROSS-VALIDATION METRICS\n",
      "+-------------------+--------+\n",
      "| Metric            |  Value |\n",
      "+-------------------+--------+\n",
      "| Mean Accuracy     | 0.6418 |\n",
      "| Mean Precision    | 0.5460 |\n",
      "| Mean Recall       | 0.6000 |\n",
      "| Mean F1           | 0.5527 |\n",
      "| Overall Accuracy  | 0.6415 |\n",
      "| Overall Precision | 0.5217 |\n",
      "| Overall Recall    | 0.6000 |\n",
      "| Overall F1        | 0.5581 |\n",
      "+-------------------+--------+\n",
      "\n",
      "AGGREGATED CONFUSION MATRIX\n",
      "+-----+-----+-----+\n",
      "|     | P:0 | P:1 |\n",
      "+-----+-----+-----+\n",
      "| A:0 |  44 |  22 |\n",
      "| A:1 |  16 |  24 |\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATE = '20251219'\n",
    "\n",
    "result_path = f'./experiments/{DATE}/flexible_gat_5fold'\n",
    "helper_utils.report_training_results(result_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1-GNN-Development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
